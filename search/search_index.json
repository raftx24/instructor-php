{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor for PHP","text":"<p>Structured data extraction in PHP, powered by LLMs. Designed for simplicity, transparency, and control.</p>"},{"location":"#what-is-instructor","title":"What is Instructor?","text":"<p>Instructor is a library that allows you to extract structured, validated data from unstructured text or OpenAI style chat sequence arrays. It is powered by Large Language Models.</p> <p>Instructor for PHP is inspired by the Instructor library for Python created by Jason Liu.</p> <p></p> <p>Here's recording of a simple CLI demo app using Instructor to extract structured data from text:</p> <p></p>"},{"location":"#feature-highlights","title":"Feature highlights","text":""},{"location":"#general","title":"General","text":"<ul> <li>Get structured responses from LLM inference</li> <li>Automate validation &amp; retries</li> </ul>"},{"location":"#flexible-inputs","title":"Flexible inputs","text":"<ul> <li>Process various types of input data: text, series of chat messages or images</li> <li>'Structured-to-structured' processing - provide object or array as an input and get object with the results of inference back</li> <li>Customize prompts and retry prompts</li> <li>Use attributes or PHP DocBlocks to provide additional instructions for LLM</li> <li>Demonstrate examples to improve the quality of inference</li> </ul>"},{"location":"#customizable-output-models","title":"Customizable output models","text":"<ul> <li>Define response data model the way to need: type-hinted classes, JSON Schema arrays, or dynamically define your data shapes with Structures</li> <li>Customize response model processing by providing your own implementation of schema, deserialization, validation and transformation interfaces</li> </ul>"},{"location":"#sync-and-streaming-support","title":"Sync and streaming support","text":"<ul> <li>Receive synchronous or streaming responses</li> <li>Get partial updates &amp; stream completed sequence items</li> </ul>"},{"location":"#observability","title":"Observability","text":"<ul> <li>Get detailed insight into internal processing via events</li> </ul>"},{"location":"#support-for-multiple-llms-api-providers","title":"Support for multiple LLMs / API providers","text":"<ul> <li>Use multiple LLM API providers (incl. OpenAI,  Anthropic, Cohere, Azure, Groq, Mistral, Anyscale, Fireworks AI, Ollama, OpenRouter, Together AI)</li> <li>Use local models with Ollama</li> </ul>"},{"location":"#documentation-and-examples","title":"Documentation and examples","text":"<ul> <li>Learn more from growing documentation and 50+ cookbooks</li> </ul>"},{"location":"#instructor-in-other-languages","title":"Instructor in Other Languages","text":"<p>Check out implementations in other languages below:</p> <ul> <li>Python (original)</li> <li>Javascript (port)</li> <li>Elixir (port)</li> </ul> <p>If you want to port Instructor to another language, please reach out to us on Twitter we'd love to help you get started!</p>"},{"location":"#how-instructor-enhances-your-workflow","title":"How Instructor Enhances Your Workflow","text":"<p>Instructor introduces three key enhancements compared to direct API usage.</p>"},{"location":"#response-model","title":"Response Model","text":"<p>You just specify a PHP class to extract data into via the 'magic' of LLM chat completion. And that's it.</p> <p>Instructor reduces brittleness of the code extracting the information from textual data by leveraging structured LLM responses.</p> <p>Instructor helps you write simpler, easier to understand code - you no longer have to define lengthy function call definitions or write code for assigning returned JSON into target data objects.</p>"},{"location":"#validation","title":"Validation","text":"<p>Response model generated by LLM can be automatically validated, following set of rules. Currently, Instructor supports only Symfony validation.</p> <p>You can also provide a context object to use enhanced validator capabilities.</p>"},{"location":"#max-retries","title":"Max Retries","text":"<p>You can set the number of retry attempts for requests.</p> <p>Instructor will repeat requests in case of validation or deserialization error up to the specified number of times, trying to get a valid response from LLM.</p>"},{"location":"#explore","title":"Explore","text":"<ul> <li>Philosophy of Instructor</li> <li>Cookbook</li> <li>Internals of Instructor</li> </ul> <p>Also, check examples in the <code>examples</code> directory of this repository for fully working, tested code examples that you can execute from the command line and see Instructor in action.</p>"},{"location":"#additional-notes","title":"Additional Notes","text":"<p>PHP ecosystem does not (yet) have a strong equivalent of Pydantic, which is at the core of Instructor for Python.</p> <p>To provide an essential functionality we needed here Instructor for PHP leverages:</p> <ul> <li>base capabilities of PHP type system,</li> <li>PHP reflection,</li> <li>PHP DocBlock type hinting conventions,</li> <li>Symfony serialization and validation capabilities</li> </ul> <p>Instructor for PHP works with OpenAI, Anthropic, local Ollama and many other providers. Check out the LLM Providers section for more information.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Instructor for PHP is compatible with PHP 8.2 or later and, due to minimal dependencies, should work with any framework of your choice.</p> <ul> <li>SaloonPHP - for handling communication with LLM API providers</li> <li>Symfony components - for validation, serialization and other utilities</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>If you want to help, check out some of the issues. They could be anything from code improvements, a guest blog post, or a new cookbook.</p>"},{"location":"data_model/","title":"Specifying Data Model","text":""},{"location":"data_model/#type-hints","title":"Type Hints","text":"<p>Use PHP type hints to specify the type of extracted data.</p> <p>Use nullable types to indicate that given field is optional.</p> <pre><code>&lt;?php\n\nclass Person {\n    public string $name;\n    public ?int $age;\n    public Address $address;\n}\n</code></pre> <p>Instructor will only fill in the fields that are public. Private and protected fields are ignored and their values are not going to be extracted (they will be left empty, with default values set as defined in your class).</p>"},{"location":"data_model/#private-vs-public-object-field","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM. Private and protected fields are left unchanged. If you want to access them directly after extraction, consider providing default values for them.</p> <p>See <code>examples/PrivateVsPublicFields/run.php</code> to check the details on the behavior of extraction for classes with private and public fields.</p>"},{"location":"data_model/#docblock-type-hints","title":"DocBlock type hints","text":"<p>You can also use PHP DocBlock style comments to specify the type of extracted data. This is useful when you want to specify property types for LLM, but can't or don't want to enforce type at the code level.</p> <pre><code>&lt;?php\n\nclass Person {\n    /** @var string */\n    public $name;\n    /** @var int */\n    public $age;\n    /** @var Address $address person's address */\n    public $address;\n}\n</code></pre> <p>See PHPDoc documentation for more details on DocBlock: https://docs.phpdoc.org/3.0/guide/getting-started/what-is-a-docblock.html#what-is-a-docblock</p>"},{"location":"data_model/#using-docblocks-as-additional-instructions-for-llm","title":"Using DocBlocks as Additional Instructions for LLM","text":"<p>You can use PHP DocBlocks (/** */) to provide additional instructions for LLM at class or field level, for example to clarify what you expect or how LLM should process your data.</p> <p>Instructor extracts PHP DocBlocks comments from class and property defined and includes them in specification of response model sent to LLM.</p> <p>Using PHP DocBlocks instructions is not required, but sometimes you may want to clarify your intentions to improve LLM's inference results.</p> <pre><code>    /**\n     * Represents a skill of a person and context in which it was mentioned. \n     */\n    class Skill {\n        public string $name;\n        /** @var SkillType $type type of the skill, derived from the description and context */\n        public SkillType $type;\n        /** Directly quoted, full sentence mentioning person's skill */\n        public string $context;\n    }\n</code></pre>"},{"location":"data_model/#attributes-for-data-model-descriptions-and-instructions","title":"Attributes for data model descriptions and instructions","text":"<p>Instructor supports <code>#[Description]</code> and <code>#[Instructions]</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p> <p><code>#[Description]</code> attribute is used to describe a class or property in your data model. Instructor will use this text to provide more context to the language model.</p> <p><code>#[Instructions]</code> attribute is used to provide additional instructions to the language model, such as how to process the data.</p> <p>You can add multiple attributes to a class or property - Instructor will merge them into a single block of text.</p> <p>Instructor will still include any PHPDoc comments provided in the class, but using attributes might be more convenient and easier to read.</p> <pre><code>&lt;?php\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make it ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    public string $job;\n}\n</code></pre>"},{"location":"data_model/#typed-collections-arrays","title":"Typed Collections / Arrays","text":"<p>PHP currently does not support generics or typehints to specify array element types.</p> <p>Use PHP DocBlock style comments to specify the type of array elements.</p> <pre><code>&lt;?php\nclass Person {\n    // ...\n}\n\nclass Event {\n    // ...\n    /** @var Person[] list of extracted event participants */\n    public array $participants;\n    // ...\n}\n</code></pre>"},{"location":"data_model/#complex-data-extraction","title":"Complex data extraction","text":"<p>Instructor can retrieve complex data structures from text. Your response model can contain nested objects, arrays, and enums.</p> <pre><code>&lt;?php\nuse Cognesy/Instructor/Instructor;\n\n// define a data structures to extract data into\nclass Person {\n    public string $name;\n    public int $age;\n    public string $profession;\n    /** @var Skill[] */\n    public array $skills;\n}\n\nclass Skill {\n    public string $name;\n    public SkillType $type;\n}\n\nenum SkillType : string {\n    case Technical = 'technical';\n    case Other = 'other';\n}\n\n$text = \"Alex is 25 years old software engineer, who knows PHP, Python and can play the guitar.\";\n\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n); // client is passed explicitly, can specify e.g. different base URL\n\n// data is extracted into an object of given class\nassert($person instanceof Person); // true\n\n// you can access object's extracted property values\necho $person-&gt;name; // Alex\necho $person-&gt;age; // 25\necho $person-&gt;profession; // software engineer\necho $person-&gt;skills[0]-&gt;name; // PHP\necho $person-&gt;skills[0]-&gt;type; // SkillType::Technical\n// ...\n\nvar_dump($person);\n// Person {\n//     name: \"Alex\",\n//     age: 25,\n//     profession: \"software engineer\",\n//     skills: [\n//         Skill {\n//              name: \"PHP\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"Python\",\n//              type: SkillType::Technical,\n//         },\n//         Skill {\n//              name: \"guitar\",\n//              type: SkillType::Other\n//         },\n//     ]\n// }\n</code></pre>"},{"location":"data_model/#dynamic-data-schemas-with-structure-class","title":"Dynamic data schemas with <code>Structure</code> class","text":"<p>See Structures for more details on how to work with dynamic data schemas.</p>"},{"location":"demonstrations/","title":"Providing examples to LLM","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>Mode::Json</code> and <code>Mode::MdJson</code> modes, where the output is expected to be a JSON object.</p> <p>Instructor's <code>request()</code> method accepts an array of examples as the <code>examples</code> parameter, where each example is an instance of the <code>Example</code> class.</p>"},{"location":"demonstrations/#example-class","title":"<code>Example</code> class","text":"<p><code>Example</code> constructor have two main arguments: <code>input</code> and <code>output</code>.</p> <p>The <code>input</code> property is  a string which describes the input message, while he <code>output</code> property is an array which represents the expected output.</p> <p>Instructor will append the list of examples to the prompt sent to LLM, with output array data rendered as JSON text.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Data\\Example;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50]\n        ),\n        new Example(\n            input: \"We have recently hired Ian, who is 27 years old.\",\n            output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n        ),\n    ],\n    mode: Mode::Json\n);\n?&gt;\n</code></pre>"},{"location":"demonstrations/#modifying-the-example-template","title":"Modifying the example template","text":"<p>You can use a template string as an input for the Example class. The template string may contain placeholders for the input data, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>{input}</code> - replaced with the actual input message  - <code>{output}</code> - replaced with the actual output data</p> <p>In case input or output data is an array, Instructor will automatically convert it to a JSON string before replacing the placeholders.</p> <pre><code>$user = (new Instructor)-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    examples: [\n        new Example(\n            input: \"John is 50 and works as a teacher.\",\n            output: ['name' =&gt; 'John', 'age' =&gt; 50],\n            template: \"EXAMPLE:\\n{input} =&gt; {output}\\n\",\n        ),\n    ],\n    mode: Mode::Json\n);\n</code></pre>"},{"location":"demonstrations/#convenience-factory-methods","title":"Convenience factory methods","text":"<p>You can also create Example instances using the <code>fromText()</code>, <code>fromChat()</code>, <code>fromData()</code> helper static methods. All of them accept $output as an array of the expected output data and differ in the way the input data is provided.</p>"},{"location":"demonstrations/#make-example-from-text","title":"Make example from text","text":"<p><code>Example::fromText()</code> method accepts a string as an input. It is equivalent to creating an instance of Example using the constructor.</p> <pre><code>$example = Example::fromText(\n    input: 'Ian is 27 yo',\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"demonstrations/#make-example-from-chat","title":"Make example from chat","text":"<p><code>Example::fromChat()</code> method accepts an array of messages, which may be useful when you want to use a chat or chat fragment as a demonstration of the input.</p> <pre><code>$example = Example::fromChat(\n    input: [['role' =&gt; 'user', 'content' =&gt; 'Ian is 27 yo']],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"demonstrations/#make-example-from-data","title":"Make example from data","text":"<p><code>Example::fromData()</code> method accepts any data type and uses the <code>Json::encode()</code> method to convert it to a string. It may be useful to provide a complex data structure as an example input.</p> <pre><code>$example = Example::fromData(\n    input: ['firstName' =&gt; 'Ian', 'lastName' =&gt; 'Brown', 'birthData' =&gt; '1994-01-01'],\n    output: ['name' =&gt; 'Ian', 'age' =&gt; 27]\n);\n</code></pre>"},{"location":"function_calls/","title":"FunctionCall helper class","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p>"},{"location":"function_calls/#extracting-arguments-for-a-function","title":"Extracting arguments for a function","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\Instructor;\n\n/** Save user data to storage */\nfunction saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: FunctionCall::fromFunctionName('saveUser'),\n);\n\n// call the function with the extracted arguments\nsaveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"function_calls/#extracting-arguments-for-a-method-call","title":"Extracting arguments for a method call","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\Instructor;\n\nclass DataStore {\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) {\n        // ...\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: FunctionCall::fromMethodName(Datastore::class, 'saveUser'),\n);\n\n// call the function with the extracted arguments\n(new DataStore)-&gt;saveUser(...$args);\n?&gt;\n</code></pre>"},{"location":"function_calls/#extracting-arguments-for-a-callable","title":"Extracting arguments for a callable","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\Instructor;\n\n/** Save user data to storage */\n$callable = function saveUser(string $name, int $age, string $country) {\n    // ...\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: FunctionCall::fromCallable($callable),\n);\n\n// call the function with the extracted arguments\n$callable(...$args);\n?&gt;\n</code></pre>"},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#material-discord-discord","title":":material-discord: Discord","text":"<p>The Discord is a great place to ask questions and get help from the community.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me @ddebowczyk or Jason Liu if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Installing Instructor is simple. Run following command in your terminal, and you're on your way to a smoother data handling experience!</p> <pre><code>composer install cognesy/instructor-php\n</code></pre>"},{"location":"llm_providers/","title":"Supported LLM Providers","text":"<p>Only tested providers with examples are listed here.</p>"},{"location":"llm_providers/#anthropic","title":"Anthropic","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::Tools</li> <li>Mode::Json (recommended)</li> <li>Mode::MdJson</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportAnthropic/run.php</code></p>"},{"location":"llm_providers/#anyscale","title":"Anyscale","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models, recommended)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportAnyscale/run.php</code></p>"},{"location":"llm_providers/#azure-openai","title":"Azure OpenAI","text":"<p>Azure is an alternative provider of OpenAI models. You can consider using it as a backup provider in case OpenAI is not available.</p> <p>Supported extraction modes:</p> <ul> <li>Mode::Tools (recommended)</li> <li>Mode::Json</li> <li>Mode::MdJson</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportAzureOAI/run.php</code></p>"},{"location":"llm_providers/#cohere","title":"Cohere","text":"<p>Cohere API is a commercial provider of Command models with their own API, which is significantly different from OpenAI. Instructor uses only a limited set of Cohere API capabilities, specifically - chat completions.</p> <p>You can find the details on how to configure the client in the example below. Cohere does not support JSON mode.</p> <p>Mode compatibility: - Mode::MdJson - supported, recommended - Mode::Tools - partially supported, not recommended (at the moment)</p> <p>Reasons Mode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportCohere/run.php</code></p>"},{"location":"llm_providers/#fireworksai","title":"FireworksAI","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportFireworksAI/run.php</code></p>"},{"location":"llm_providers/#groq","title":"Groq","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (recommended)</li> <li>Mode::Tools (experimental - not stable)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportGroq/run.php</code></p>"},{"location":"llm_providers/#mistral-api","title":"Mistral API","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportMistral/run.php</code></p>"},{"location":"llm_providers/#ollama","title":"Ollama","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models, recommended)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportOllama/run.php</code></p>"},{"location":"llm_providers/#openai","title":"OpenAI","text":"<p>OpenAI is the default provider that is called by Instructor unless user configures different one.</p> <p>OpenAI models (esp. GPT-4 and newer) perform very well across all 3 extraction modes.  </p> <p>Supported extraction modes:  - Mode::Tools (recommended)  - Mode::Json  - Mode::MdJson</p> <p>Majority of examples use OpenAI provider.</p> <ul> <li><code>./examples/05_APISupport/LLMSupportOpenAI/run.php</code></li> </ul>"},{"location":"llm_providers/#openrouter","title":"OpenRouter","text":"<p>You have to use our client adapter to work around the problem in the response format returned by OpenRouter for non-streamed requests.</p> <p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example:  - <code>./examples/05_APISupport/LLMSupportOpenRouter/run.php</code></p>"},{"location":"llm_providers/#togetherai","title":"TogetherAI","text":"<p>Supported extraction modes:</p> <ul> <li>Mode::MdJson</li> <li>Mode::Json (for selected models)</li> <li>Mode::Tools (for selected models)</li> </ul> <p>Example: - <code>./examples/05_APISupport/LLMSupportTogetherAI/run.php</code></p>"},{"location":"model_options/","title":"LLM model and options","text":""},{"location":"model_options/#changing-llm-model-and-options","title":"Changing LLM model and options","text":"<p>You can specify model and other options that will be passed to OpenAI / LLM endpoint.</p> <pre><code>&lt;?php\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: [\n        // custom temperature setting\n        'temperature' =&gt; 0.0\n        // ... other options\n    ],\n);\n</code></pre>"},{"location":"model_options/#providing-custom-client","title":"Providing custom client","text":"<p>You can pass a custom configured instance of client to the Instructor. This allows you to specify your own API key, base URI or organization.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Utils\\Env;\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.openai.com/v1',\n    organization: '',\n    connectTimeout: 3,\n    requestTimeout: 30,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$person = $instructor-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n    model: 'gpt-3.5-turbo',\n    options: ['temperature' =&gt; 0.0],\n);\n</code></pre>"},{"location":"modes/","title":"Extraction modes","text":"<p>Instructor supports several ways to extract data from the response. The default mode is <code>Mode::Tools</code>, which leverages OpenAI-style tool calls.</p> <p>Mode can be set via parameter of <code>Instructor::response()</code> or <code>Instructor::request()</code> methods.</p> <p><pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$instructor = new Instructor();\n\n$response = $instructor-&gt;respond(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: Mode::Json\n);\n</code></pre> Mode can be also set via <code>request()</code> method.</p> <pre><code>&lt;?php\n$response = $instructor-&gt;request(\n    messages: \"...\",\n    responseModel: ...,\n    ...,\n    mode: Mode::Json\n)-&gt;get();\n</code></pre>"},{"location":"modes/#modes","title":"Modes","text":""},{"location":"modes/#modetools","title":"<code>Mode::Tools</code>","text":"<p>This mode is the default one. It uses OpenAI tools to extract data from the response.</p> <p>It is the most reliable mode, but not all models and API providers support it - check their documentation for more information.</p>"},{"location":"modes/#modejson","title":"<code>Mode::Json</code>","text":"<p>In this mode Instructor provides response format as JSONSchema and asks LLM to respond with JSON object following provided schema.</p> <p>It is supported by many open source models and API providers - check their documentation.</p> <p>See <code>response_mode</code> in (OpenAI API Reference)[https://platform.openai.com/docs/api-reference/chat/create]</p>"},{"location":"modes/#modemdjson","title":"<code>Mode::MdJson</code>","text":"<p>In this mode Instructor asks LLM to answer with JSON object following provided schema and return answer as Markdown codeblock.</p> <p>It may improve the results for LLMs that have not been finetuned to respond with JSON as they are likely to be already trained on large amounts of programming docs and have seen a lot of properly formatted JSON objects within MD codeblocks.</p>"},{"location":"modules/","title":"Modules","text":"<p>NOTE: This is a work in progress. The documentation is not complete yet.</p> <p>Modules are a way to encapsulate structured processing logic and data flows. They are inspired by DSPy and TensorFlow modules.</p> <p>Instructor comes with a set of built-in modules, which can be used to build more complex processing pipelines. You can also create your own modules, by extending provided classes (<code>Module</code> or <code>DynamicModule</code>) or implementing interfaces (<code>CanProcessCall</code>, <code>HasPendingExecution</code>, <code>HasInputOutputSchema</code>).</p>"},{"location":"modules/#module-anatomy","title":"Module anatomy","text":"<p>Module consist of 3 important parts:</p> <ul> <li><code>__construct()</code> - constructor containing the setup of the module components and dependencies,</li> <li><code>signature()</code> - method returning the signature of the module, which specified expected inputs and resulting output fields,</li> <li><code>forward()</code> - method containing the processing logic, which takes the input data and returns the output data, using the module components configured in the constructor.</li> </ul>"},{"location":"modules/#signatures","title":"Signatures","text":"<p>Signatures are a way to define the expected inputs and resulting outputs of a module. They are also used to validate the input data and to infer the output data.</p> <p>Signature of the module returned by <code>signature()</code> method can be defined in several ways.</p> <ul> <li>you can just return a string in a form of <code>input1: type, input2: type -&gt; output1: type1, output2: type2</code></li> <li>you can return an instance of a class implementing <code>HasSignature</code> interface (which has <code>signature()</code> method returning the signature string)</li> <li>as an instance of <code>Signature</code> class</li> </ul> <p>String based signature is the simplest way to define the signature, but it's less flexible and may be harder to maintain, especially in more complex cases.</p> <p><code>SignatureData</code> base class is more flexible way to define the inputs and outputs of a module, which can be useful in more complex cases.</p> <p>Extend <code>SignatureData</code> class and define the fields using <code>#[InputField]</code> and <code>#[OutputField]</code> attributes. The fields can have type hints, which are used to validate the input data. Also, <code>#[InputField]</code> and <code>#[OutputField]</code> attributes can contain instructions for LLM, specifying the inference behavior.</p>"},{"location":"modules/#calling-the-module","title":"Calling the module","text":"<p>Initiation of the module with the input data is done via <code>withArgs()</code> or <code>with()</code> methods. - <code>withArgs()</code> - takes the input data fields as arguments - they have to be named arguments - <code>with()</code> - takes the input data as an object implementing <code>HasInputOutputData</code> interface - can be used if the module has class based signature</p> <p><code>withArgs()</code> and <code>with()</code> methods available on any <code>Module</code> class take the input data, and create <code>PendingExecution</code> object, which is executed when you access the results via <code>result()</code> or <code>get()</code> methods.</p>"},{"location":"modules/#working-with-results","title":"Working with results","text":"<p>Results of the calling the module via <code>with()</code> or <code>withArgs()</code> is an instance of <code>PendingExecution</code> object, containing the ways to access module outputs.</p> <p><code>PendingExecution</code> object offers several methods to access the output data:</p> <ul> <li><code>result()</code> - returns the raw output of the module as defined by <code>forward()</code> method,</li> <li><code>try()</code> - returns the output of the module as a <code>Result</code> object which is a wrapper around the output data, which can be used to check if the output is valid or if there are any errors before accessing the data, </li> <li><code>get(string $name)</code> - returns the value of specified output field,</li> <li><code>get()</code> - returns the output data as an array of key-value pairs of field name =&gt; field value.</li> </ul> <p>Additionally, <code>PendingExecution</code> object offers following methods:</p> <ul> <li><code>errors()</code> - returns the list of errors that occurred during the execution of the module,</li> <li><code>hasErrors()</code> - returns <code>true</code> if there have been any errors encountered during execution of the module.</li> </ul>"},{"location":"partials/","title":"Partial updates and streaming","text":""},{"location":"partials/#partial-updates","title":"Partial updates","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated.</p> <p>You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p>This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive partial results define <code>onPartialUpdate()</code> callback that will be called on every update of the deserializad object.</p> <p>Instructor is smart about updates, it calculates and compares hashes of the previous and newly  deserialized version of the model, so you won't get them on every token received, but only when any property of the object is updated.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\nfunction updateUI($person) {\n    // Here you get partially completed Person object update UI with the partial result\n}\n\n$person = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;onPartialUpdate(\n    fn($partial) =&gt; updateUI($partial)\n)-&gt;get();\n\n// Here you get completed and validated Person object\n$this-&gt;db-&gt;save($person); // ...for example: save to DB\n</code></pre> <p>Partially updated data is not validated while they are received and deserialized.</p> <p>The object returned from <code>get()</code> call is fully validated, so you can safely work with it, e.g. save it to the database.</p>"},{"location":"partials/#streaming-responses","title":"Streaming responses","text":"<p>You can get a stream of responses by setting the <code>stream</code> option to <code>true</code> and calling the <code>stream()</code> method instead of <code>get()</code>. It returns <code>Stream</code> object, which gives you access to the response streamed from LLM and processed by Instructor into structured data.</p> <p>Following methods are available to process the stream:</p> <ul> <li><code>partials()</code>: Returns a generator of partial updates from the stream. Only final update is validated, partial updates are only deserialized and transformed.</li> <li><code>sequence()</code>: Dedicated to processing <code>Sequence</code> response models - returns only completed items in the sequence.</li> <li><code>getLastUpdate()</code>: Returns the last object received and processed by Instructor.</li> </ul> <p>One more method available on <code>Stream</code> is <code>final()</code>. It returns only the final response object. It blocks until the response is fully processed. It is typically used when you only need final result and prefer to use <code>onPartialUpdate()</code> or <code>onSequenceUpdate()</code> to process partial updates. It's an equivalent to calling <code>get()</code> method (but requires <code>stream</code> option set to <code>true</code>).</p>"},{"location":"partials/#example-streaming-partial-responses","title":"Example: streaming partial responses","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$stream = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $update) {\n    // render updated person view\n    // for example:\n    $view-&gt;updateView($update); // render the updated person view\n}\n\n// now you can get final, fully processed person object\n$person = $stream-&gt;getLastUpdate();\n// ...and for example save it to the database\n$db-&gt;savePerson($person);\n</code></pre>"},{"location":"partials/#example-streaming-sequence-items","title":"Example: streaming sequence items","text":"<pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$stream = (new Instructor)-&gt;request(\n    messages: \"Jason is 28 years old, Amanda is 26 and John (CEO) is 40.\",\n    responseModel: Sequence::of(Participant::class),\n)-&gt;stream();\n\nforeach ($stream-&gt;sequence() as $update) {\n    // append last completed item from the sequence\n    // for example:\n    $view-&gt;appendParticipant($update-&gt;last());\n}\n\n// now you can get final, fully processed sequence of participants\n$participants = $stream-&gt;getLastUpdate();\n// ...and for example save it to the database\n$db-&gt;saveParticipants($participants-&gt;toArray());\n</code></pre>"},{"location":"prompts/","title":"Customizing prompts","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>request()</code> or <code>respond()</code> methods.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p>"},{"location":"prompts/#prompting-models-with-tool-calling-support","title":"Prompting models with tool calling support","text":"<p><code>Mode::Tools</code> is usually most reliable way to get structured outputs following provided response schema.</p> <p><code>Mode::Tools</code> can make use of <code>$toolName</code> and <code>$toolDescription</code> parameters to provide additional semantic context to the LLM, describing the tool to be used for processing the input. <code>Mode::Json</code> and <code>Mode::MdJson</code> ignore these parameters, as tools are not used in these modes.</p> <pre><code>&lt;?php\n$user = (new Instructor)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        toolName: 'extract',\n        toolDescription: 'Extract information from provided content',\n        mode: Mode::Tools)\n    -&gt;get();\n</code></pre>"},{"location":"prompts/#prompting-models-supporting-json-output","title":"Prompting models supporting JSON output","text":"<p>Aside from tool calling Instructor supports two other modes for getting structured outputs from LLM: <code>Mode::Json</code> and <code>Mode::MdJson</code>.</p> <p><code>Mode::Json</code> uses JSON mode offered by some models and API providers to get LLM respond in JSON format rather than plain text.</p> <p><pre><code>&lt;?php\n$user = (new Instructor)-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    prompt: \"\\nYour task is to respond correctly with JSON object.\",\n    mode: Mode::Json\n);\n</code></pre> Note that various models and API providers have specific requirements on the input format, e.g. for OpenAI JSON mode you are required to include <code>JSON</code> string in the prompt.</p>"},{"location":"prompts/#including-json-schema-in-the-prompt","title":"Including JSON Schema in the prompt","text":"<p>Instructor takes care of automatically setting the <code>response_format</code> parameter, but this may not be sufficient for some models or providers - some of them require specifying JSON response format as part of the prompt, rather than just as <code>response_format</code> parameter in the request (e.g. OpenAI).</p> <p>For this reason, when using Instructor's <code>Mode::Json</code> and <code>Mode::MdJson</code> you should include the expected JSON Schema in the prompt. Otherwise, the response is unlikely to match your target model, making it impossible for Instructor to deserialize it correctly.</p> <pre><code>&lt;?php\n$jsonSchema = json_encode([\n    \"type\" =&gt; \"object\",\n    \"properties\" =&gt; [\n        \"name\" =&gt; [\"type\" =&gt; \"string\"],\n        \"age\" =&gt; [\"type\" =&gt; \"integer\"]\n    ],\n    \"required\" =&gt; [\"name\", \"age\"]\n]);\n\n$user = $instructor\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema: $jsonSchema\\n\",\n        mode: Mode::Json)\n    -&gt;get();\n</code></pre> <p>The example above demonstrates how to manually create JSON Schema, but with Instructor you do not have to build the schema manually - you can use prompt template placeholder syntax to use Instructor-generated JSON Schema.</p>"},{"location":"prompts/#prompt-as-template","title":"Prompt as template","text":"<p>Instructor allows you to use a template string as a prompt. You can use <code>&lt;|variable|&gt;</code> placeholders in the template string, which will be replaced with the actual values during the execution.</p> <p>Currently, the following placeholders are supported:  - <code>&lt;|json_schema|&gt;</code> - replaced with the JSON Schema for current response model</p> <p>Example below demonstrates how to use a template string as a prompt:</p> <pre><code>&lt;?php\n$user = (new Instructor)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: Mode::Json)\n    -&gt;get();\n</code></pre>"},{"location":"prompts/#prompting-the-models-with-no-support-for-tool-calling-or-json-output","title":"Prompting the models with no support for tool calling or JSON output","text":"<p><code>Mode::MdJson</code> is the most basic (and least reliable) way to get structured outputs from LLM. Still, you may want to use it with the models which do not support tool calling or JSON output.</p> <p><code>Mode::MdJson</code> relies on the prompting to get LLM response in JSON formatted data.</p> <p>Many models prompted in this mode will respond with a mixture of plain text and JSON data. Instructor will try to find JSON data fragment in the response and ignore the rest of the text.</p> <p>This approach is most prone to deserialization and validation errors and needs providing JSON Schema in the prompt to increase the probability that the response is correctly structured and contains the expected data.</p> <pre><code>&lt;?php\n$user = (new Instructor)\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n{json_schema}\\n\",\n        mode: Mode::MdJson)\n    -&gt;get();\n</code></pre>"},{"location":"scalars/","title":"Extracting Scalar Values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::integer('age'),\n);\n\nvar_dump($value);\n// int(28)\n</code></pre> <p>In this example, we're extracting a single integer value from the text. You can also use <code>Scalar::string()</code>, <code>Scalar::boolean()</code> and <code>Scalar::float()</code> to extract other types of values.</p> <p>Additionally, you can use Scalar adapter to extract enums via <code>Scalar::enum()</code>.</p>"},{"location":"scalars/#examples","title":"Examples","text":""},{"location":"scalars/#string-result","title":"String result","text":"<pre><code>&lt;?php\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::string(name: 'firstName'),\n);\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe(\"Jason\");\n</code></pre>"},{"location":"scalars/#integer-result","title":"Integer result","text":"<pre><code>&lt;?php\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::integer('age'),\n);\n// expect($value)-&gt;toBeInt();\n// expect($value)-&gt;toBe(28);\n</code></pre>"},{"location":"scalars/#boolean-result","title":"Boolean result","text":"<pre><code>&lt;?php\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Scalar::boolean(name: 'isAdult'),\n);\n// expect($value)-&gt;toBeBool();\n// expect($value)-&gt;toBe(true);\n</code></pre>"},{"location":"scalars/#float-result","title":"Float result","text":"<pre><code>&lt;?php\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old and his 100m sprint record is 11.6 seconds.\",\n    responseModel: Scalar::float(name: 'recordTime'),\n);\n// expect($value)-&gt;toBeFloat();\n// expect($value)-&gt;toBe(11.6);\n</code></pre>"},{"location":"scalars/#enum-result-select-one-of-the-options","title":"Enum result / select one of the options","text":"<pre><code>&lt;?php\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$value = (new Instructor)-&gt;respond(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; $text],\n        ['role' =&gt; 'user', 'content' =&gt; 'What is Jason\\'s citizenship?'],\n    ],\n    responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n);\n// expect($value)-&gt;toBeString();\n// expect($value)-&gt;toBe('other');\n</code></pre>"},{"location":"sequences/","title":"Sequences","text":""},{"location":"sequences/#extracting-sequences-of-objects","title":"Extracting Sequences of Objects","text":"<p>Sequence is a wrapper class that can be used to represent a list of objects to be extracted by Instructor from provided context.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Sequence::of(Person::class),\n);\n</code></pre>"},{"location":"sequences/#streaming-sequences","title":"Streaming Sequences","text":"<p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <p>NOTE This feature requires the <code>stream</code> option to be set to <code>true</code>.</p> <p>To receive sequence updates provide a callback via Instructor's <code>onSequenceUpdate()</code> that will be called each  time a new item is received from LLM.</p> <p>The callback provided a full sequence that has been retrieved so far. You can get the last added object from the sequence via <code>$sequence-&gt;last()</code>.</p> <p>Remember that while the sequence is being updated, the data is not validated - only when the sequence is fully extracted, the objects are validated and a full sequence is returned (see example below).</p> <pre><code>&lt;?php\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\nfunction updateUI(Person $person) {\n    // add newly extracted person to the UI list\n    $this-&gt;ui-&gt;appendToList($person);\n    // remember those objects are not validated yet\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old\n    and Anna is 2 years younger than him.\nTEXT;\n\n$list = (new Instructor)-&gt;request(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Sequence::of(Person::class),\n    options: ['stream' =&gt; true]\n)-&gt;onSequenceUpdate(\n    fn($sequence) =&gt; updateUI($sequence-&gt;last()) // get last added object\n)-&gt;get();\n\n// now the list is fully extracted and validated\nforeach ($list as $person) {\n    // do something with each person\n    $this-&gt;db-&gt;save($person);\n}\n</code></pre>"},{"location":"sequences/#working-with-sequences","title":"Working with Sequences","text":"<p>Sequences offer array access (via ArrayAccess) and convenience methods to work with the list of extracted objects.</p> <pre><code>&lt;?php\n$sequence-&gt;count();   // returns the number of extracted items\n$sequence-&gt;first();   // returns the first extracted item\n$sequence-&gt;last();    // returns the last extracted item\n$sequence-&gt;get(1);    // returns the second extracted item\n$sequence-&gt;toArray(); // returns the list of extracted items as an array\n</code></pre>"},{"location":"sequences/#streaming-sequence-updates","title":"Streaming sequence updates","text":"<p>See: Streaming and partial updates for more information on how to get partial updates and streaming of sequences.</p>"},{"location":"structures/","title":"Structures","text":"<p>If you want to define the shape of data during runtime, you can use <code>Structure</code> class.</p> <p>Structures allow you to define and modify arbitrary shape of data to be extracted by LLM. Classes may not be the best fit for this purpose, as declaring or changing them during execution is not possible.</p> <p>With structures, you can define custom data shapes dynamically, for example based on the user input or context of the processing, to specify the information you need LLM to infer from the provided text or chat messages.</p>"},{"location":"structures/#defining-a-shape-of-data","title":"Defining a shape of data","text":"<p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p><code>Structure::define()</code> accepts array of <code>Field</code> objects.</p> <p>Let's first define the structure, which is a shape of the data we want to extract from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Extras\\Structure\\Field;\nuse Cognesy\\Instructor\\Extras\\Structure\\Structure;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::enum('role', Role::class),\n]);\n?&gt;\n</code></pre> <p>Following types of fields are currently supported:</p> <ul> <li><code>Field::bool()</code> - boolean value</li> <li><code>Field::int()</code> - int value</li> <li><code>Field::string()</code> - string value</li> <li><code>Field::float()</code> - float value</li> <li><code>Field::enum()</code> - enum value</li> <li><code>Field::structure()</code> - for nesting structures</li> </ul>"},{"location":"structures/#optional-fields","title":"Optional fields","text":"<p>Fields can be marked as optional with <code>$field-&gt;optional()</code>.  By default, all defined fields are required.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    //...\n    Field::int('age')-&gt;optional(),\n    //...\n]);\n?&gt;\n</code></pre>"},{"location":"structures/#descriptions-for-guiding-llm-inference","title":"Descriptions for guiding LLM inference","text":"<p>Instructor includes field descriptions in the content of instructions for LLM, so you can use them to provide explanations, detailed specifications or requirements for each field.</p> <p>You can also provide extra inference instructions for LLM at the structure level with <code>$structure-&gt;description(string $description)</code></p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name', 'Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;optional(),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"structures/#nesting-structures","title":"Nesting structures","text":"<p>You can use <code>Field::structure()</code> to nest structures in case you want to define more complex data shapes.</p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"structures/#validation-of-structure-data","title":"Validation of structure data","text":"<p>Instructor supports validation of structures.</p> <p>You can define field validator with:</p> <ul> <li><code>$field-&gt;validator(callable $validator)</code> - $validator has to return an instance of <code>ValidationResult</code></li> <li><code>$field-&gt;validIf(callable $condition, string $message)</code> - $condition has to return false if validation has not succeeded, $message with be provided to LLM as explanation for self-correction of the next extraction attempt</li> </ul> <p>Let's add a simple field validation to the example above: </p> <pre><code>&lt;?php\n$structure = Structure::define('person', [\n    // ...\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    // ...\n], 'A person object');\n?&gt;\n</code></pre>"},{"location":"structures/#extracting-data","title":"Extracting data","text":"<p>Now, let's extract the data from the message.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield. She is 25 years old and works as a line worker. \n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345.\n    TEXT;\n\n$person = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: $structure,\n);\n\ndump($person-&gt;toArray());\n// array [\n//   \"name\" =&gt; \"Jane Doe\"\n//   \"age\" =&gt; 25\n//   \"address\" =&gt; array [\n//     \"city\" =&gt; \"Springfield\"\n//   ]\n//   \"role\" =&gt; \"line\"\n// ]\n?&gt;\n</code></pre>"},{"location":"structures/#working-with-structure-objects","title":"Working with <code>Structure</code> objects","text":"<p>Structure object properties can be accessed using <code>get()</code> and <code>set()</code> methods, but also directly as properties.</p> <pre><code>&lt;?php\n$person = Structure::define('person', [\n    Field::string('name'),\n    Field::int('age'),\n    Field::structure('role', [\n        Field::string('name'),\n        Field::int('level'),\n    ])\n]);\n\n// Setting properties via set()\n$person-&gt;set('name', 'John Doe');\n$person-&gt;set('age', 30);\n$person-&gt;get('role')-&gt;set('name', 'Manager');\n$person-&gt;get('role')-&gt;set('level', 1);\n\n// Setting properties directly \n$person-&gt;name = 'John Doe';\n$person-&gt;age = 30;\n$person-&gt;role-&gt;name = 'Manager';\n$person-&gt;role-&gt;level = 1;\n\n// Getting properties via get()\n$name = $person-&gt;get('name');\n$age = $person-&gt;get('age');\n$role = $person-&gt;get('role')-&gt;get('name');\n$level = $person-&gt;get('role')-&gt;get('level');\n\n// Getting properties directly\n$name = $person-&gt;name;\n$age = $person-&gt;age;\n$role = $person-&gt;role-&gt;name;\n$level = $person-&gt;role-&gt;level;\n?&gt;\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#basic-usage","title":"Basic usage","text":"<p>This is a simple example demonstrating how Instructor retrieves structured information from provided text (or chat message sequence).</p> <p>Response model class is a plain PHP class with typehints specifying the types of fields of the object.</p> <p>NOTE: By default, Instructor looks for OPENAI_API_KEY environment variable to get your API key. You can also provide the API key explicitly when creating the Instructor instance.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n// Step 0: Create .env file in your project root:\n// OPENAI_API_KEY=your_api_key\n\n// Step 1: Define target data structure(s)\nclass Person {\n    public string $name;\n    public int $age;\n}\n\n// Step 2: Provide content to process\n$text = \"His name is Jason and he is 28 years old.\";\n\n// Step 3: Use Instructor to run LLM inference\n$person = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Person::class,\n);\n\n// Step 4: Work with structured response data\nassert($person instanceof Person); // true\nassert($person-&gt;name === 'Jason'); // true\nassert($person-&gt;age === 28); // true\n\necho $person-&gt;name; // Jason\necho $person-&gt;age; // 28\n\nvar_dump($person);\n// Person {\n//     name: \"Jason\",\n//     age: 28\n// }    \n</code></pre> <p>Note</p> <p>Currently, Instructor for PHP only supports classes / objects as response models. In case you want to extract simple types or arrays, you need to wrap them in a class (or use <code>Scalar</code> helper class).</p>"},{"location":"usage/#string-as-input","title":"String as Input","text":"<p>You can provide a string instead of an array of messages. This is useful when you want to extract data from a single block of text and want to keep your code simple.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\n\n$value = (new Instructor)-&gt;respond(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n</code></pre>"},{"location":"usage/#alternative-way-to-get-results","title":"Alternative way to get results","text":"<p>You can call <code>request()</code> method to initiate Instructor with request data and then call <code>get()</code> to get the response.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\n\n$instructor = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n);\n\n$person = $instructor-&gt;get();\n</code></pre>"},{"location":"usage/#structured-to-structured-data-processing","title":"Structured-to-structured data processing","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>respond()</code> and <code>request()</code> methods can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translation = (new Instructor)-&gt;respond(\n    input: $email,\n    responseModel: Email::class,\n    prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n);\n?&gt;\n</code></pre>"},{"location":"usage/#streaming-support","title":"Streaming support","text":"<p>Instructor supports streaming of partial results, allowing you to start processing the data as soon as it is available.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n$stream = (new Instructor)-&gt;request(\n    messages: \"His name is Jason, he is 28 years old.\",\n    responseModel: Person::class,\n    options: ['stream' =&gt; true]\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $partialPerson) {\n    // process partial person data\n    echo \"Name: \" $partialPerson-&gt;name ?? '...';\n    echo \"Age: \" $partialPerson-&gt;age ?? '...';\n}\n\n// after streaming is done you can get the final, fully processed person object...\n$person = $stream-&gt;getLastUpdate()\n// ...to, for example, save it to the database\n$db-&gt;save($person);\n?&gt;\n</code></pre>"},{"location":"usage/#scalar-responses","title":"Scalar responses","text":"<p>See Scalar responses for more information on how to generate scalar responses with <code>Scalar</code> adapter class.</p>"},{"location":"usage/#partial-responses-and-streaming","title":"Partial responses and streaming","text":"<p>See Streaming and partial updates for more information on how to work with partial updates and streaming.</p>"},{"location":"usage/#extracting-arguments-for-function-call","title":"Extracting arguments for function call","text":"<p>See FunctionCall helper class for more information on how to extract arguments for callable objects. ```</p>"},{"location":"validation/","title":"Validation","text":""},{"location":"validation/#basic-validation","title":"Basic validation","text":"<p>Instructor validates results of LLM response against validation rules specified in your data model.</p> <p>Note</p> <p>For further details on available validation rules, check Symfony Validation constraints.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is Jason, he is -28 years old.\";\n\n$person = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: Person::class,\n);\n\n// if the resulting object does not validate, Instructor throws an exception\n</code></pre>"},{"location":"validation/#max-retries","title":"Max Retries","text":"<p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <pre><code>&lt;?php\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass Person {\n    #[Assert\\Length(min: 3)]\n    public string $name;\n    #[Assert\\PositiveOrZero]\n    public int $age;\n}\n\n$text = \"His name is JX, aka Jason, he is -28 years old.\";\n\n$person = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: Person::class,\n    maxRetries: 3,\n);\n\n// if all LLM's attempts to self-correct the results fail, Instructor throws an exception\n</code></pre>"},{"location":"validation/#custom-validation","title":"Custom Validation","text":"<p>You can easily add custom validation code to your response model by using <code>ValidationTrait</code> and defining validation logic in <code>validate()</code> method.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $age;\n\n    public function validate() : array {\n        if ($this-&gt;name === strtoupper($this-&gt;name)) {\n            return [];\n        }\n        return [[\n            'message' =&gt; \"Name must be in uppercase.\",\n            'path' =&gt; 'name',\n            'value' =&gt; $this-&gt;name\n        ]];\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note that method <code>validate()</code> has to return:  * an empty array if the object is valid,  * or an array of validation violations.</p> <p>This information will be used by LLM to make subsequent attempts to correct the response.</p> <pre><code>$violations = [\n    [\n        'message' =&gt; \"Error message with violation details.\",\n        'path' =&gt; 'path.to.property',\n        'value' =&gt; '' // invalid value\n    ],\n    // ...other violations\n];\n</code></pre>"},{"location":"validation/#custom-validation-via-symfony-assertcallback","title":"Custom Validation via Symfony #[Assert/Callback]","text":"<p>Instructor uses Symfony validation component to validate extracted data.</p> <p>You can use <code>#[Assert/Callback]</code> annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;name === \"JASON\");\n</code></pre> <p>Note</p> <p>See Symfony docs for more details on how to use Callback constraint.</p>"},{"location":"why/","title":"Why use Instructor?","text":"<p>Our library introduces three key enhancements:</p> <ul> <li>Response Mode: Specify a PHP model to streamline data extraction.</li> <li>Validation: Validates response generated by LLM.</li> <li>Max Retries: Set your desired number of retry attempts for requests.</li> </ul>"},{"location":"why/#a-glimpse-into-instructors-capabilities","title":"A Glimpse into Instructor's Capabilities","text":"<p>With Instructor, your code becomes more efficient and readable. Here\u2019s a quick peek.</p>"},{"location":"why/#understanding-the-workflow","title":"Understanding the workflow","text":"<p>Let's see how we can leverage it to make use of instructor</p>"},{"location":"why/#step-1-define-the-data-model","title":"Step 1: Define the data model","text":"<p>Create a data model to define the structure of the data you want to extract. This model will map directly to the information in the prompt.</p> <pre><code>&lt;?php\nclass UserDetail {\n    public string $name;\n    public int $age;\n}\n</code></pre>"},{"location":"why/#step-2-extract","title":"Step 2: Extract","text":"<p>Use the <code>Instructor::respond()</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies the model to use for extraction.</p> <pre><code>/** @var UserDetail */\n$user = (new Instructor)-&gt;respond(\n    messages: [[\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"]],\n    responseModel: UserDetail::class,\n    model: \"gpt-3.5-turbo\",\n);\n\nassert($user-&gt;name == \"Jason\")\nassert($user-&gt;age == 25)\n</code></pre> <p>It's helpful to annotate the variable with the type of the response model, which will help your IDE provide autocomplete and spell check.</p>"},{"location":"why/#understanding-validation","title":"Understanding Validation","text":"<p>Validation can also be plugged into the same data model. If the response triggers any validation rules Instructor will raise a validation error.</p>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Here, the <code>LeadReport</code> model is passed as the <code>$responseModel</code>, and <code>$maxRetries</code> is set to 2. It means that if the extracted data does not match the model, Instructor will re-ask the model 2 times before giving up.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via jason@gmailcom -- Jason\"]],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;email === \"jason@gmail.com\");\n</code></pre> <p>More about Validation</p> <p>Check out Jason's blog post Good LLM validation is just good validation</p>"},{"location":"why/#custom-validators","title":"Custom Validators","text":"<p>Instructor uses Symfony validation component to validate extracted data. You can use #[Assert/Callback] annotation to build fully customized validation logic.</p> <p>See Symfony docs for more details on how to use Callback constraint.</p> <pre><code>use Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in uppercase.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\nassert($user-&gt;name === \"JASON\");\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"concepts/classification/","title":"Text Classification using LLM","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using LLM (via OpenAI API), PHP's <code>enums</code> and classes.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using language models in combination with PHP data structures.</p>"},{"location":"concepts/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"concepts/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n// Enumeration for single-label text classification. \nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction. \nclass SinglePrediction {\n    public Label $classLabel;\n}\n</code></pre>"},{"location":"concepts/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Perform single-label classification on the input text. \n */\nfunction classify(string $data) : SinglePrediction {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n        model: \"gpt-3.5-turbo-0613\",\n    );\n}\n</code></pre>"},{"location":"concepts/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\nassert($prediction-&gt;classLabel == Label::SPAM);\n</code></pre>"},{"location":"concepts/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"concepts/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass Ticket {\n    /** @var Label[] */\n    public array $ticketLabels = [];\n}\n</code></pre>"},{"location":"concepts/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : Ticket {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify following support ticket: {$data}\",\n        ]],\n        responseModel: Ticket::class,\n        model: \"gpt-3.5-turbo-0613\",\n    );\n}\n</code></pre>"},{"location":"concepts/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;classLabels));\nassert(in_array(Label::BILLING, $prediction-&gt;classLabels));\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>Note</p> <p>Philosophy behind Instructor was formulated by Jason Liu, the creator of original version of Instructor in Python and adapted for the PHP port.</p> <p>Instructor values simplicity and flexibility in leveraging language models. It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions.</p> <p>\u201cSimplicity is a great virtue, but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#simplicity","title":"Simplicity","text":"<ol> <li>Most users will only need to learn <code>responseModel</code> and <code>Instructor::respond()</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"concepts/philosophy/#transparency","title":"Transparency","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We give you config over the prompts we do write ('reasking' and in the future - JSON_MODE prompts).</li> </ol>"},{"location":"concepts/philosophy/#flexibility","title":"Flexibility","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt Instructor by just adding <code>Instructor::respond()</code> with data schemas fed in via <code>responseModel</code>.</li> <li>Use any class to define your data schema (no need to inherit from some base class).</li> </ol>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of PHP classes, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define data schema <code>&lt;?php class StructuredData { ... }</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>&lt;?php function extract($input) : StructuredData</code></li> <li>Define typed computations against your data with <code>&lt;?php function compute(StructuredData $data):</code> or call methods on your schema <code>&lt;?php $data-&gt;compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#our-goals","title":"Our Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better programmer and, as a result, a better AI engineer.</p> <ul> <li>The library is a result of our desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>We won't try to write prompts for you,</li> <li>We don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit us up @jnxlco or @ddebowczyk</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor for function calling is to make the models self-descriptive, modular, and flexible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use PHPDoc comments or #[Description('')] attribute for clear field descriptions.</li> <li>Optionality: Use PHP's nullable types (e.g. ?int) for optional fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#utilize-nullable-attribute","title":"Utilize Nullable Attribute","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/OptionalFields/run.php</code></p> <p>Use PHP's nullable types by prefixing type name with question mark (?) and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?Role $role = null; \n}\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/HandlingErrors/run.php</code></p> <p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?string $role = null;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $result = null;\n    public ?string $errorMessage = '';\n    public bool $error = false;\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;error ? null : $this-&gt;result;\n    }\n}\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in 'errorMessage'.</p> <p>Original Instructor implementation in Python provides utility class Maybe making this pattern even easier. Such mechanism is not yet available in PHP version of Instructor.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>&lt;?php\nenum Role : string {\n    case Principal = 'principal'\n    case Teacher = 'teacher'\n    case Student = 'student'\n    case Other = 'other'\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /**  Correctly assign one of the predefined roles to the user. */\n    public Role $role;\n}\n</code></pre> <p>If you'd like to improve LLM inference performance, try reiterating the requirements in the field descriptions (in the docstrings).</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/RestatingInstruction/run.php</code></p> <p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>&lt;?php\n/** Extract the role based on the following rules: &lt;your rules go here&gt; */\nclass Role\n{\n    /** Restate the instructions and rules to correctly determine the title. */\n    public string $instructions;\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ArbitraryProperties/run.php</code></p> <p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/LimitingLengthOfLists/run.php</code></p> <p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>&lt;?php\nclass Property\n{\n    /**  Monotonically increasing ID */\n    public string $index; \n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age\n    public string $name;\n    /** @var Property[] Numbered list of arbitrary extracted properties, should be less than 3 */\n    public array $properties;\n}\n</code></pre> <p>To be 100% certain the list does not exceed the limit add extra validation, e.g. using ValidationMixin (see: Validation).</p>"},{"location":"concepts/prompting/#consistent-arbitrary-properties","title":"Consistent Arbitrary Properties","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ArbitraryPropertiesConsistency/run.php</code></p> <p>For multiple records containing arbitrary properties, instruct LLM to use consistent key names when extracting properties.</p> <pre><code>&lt;?php\nclass Property {\n    public int $id;\n    public string $key;\n    public string $name;\n}\n\nclass UserDetails\n{\n    /** @var UserDetail[] Extract information for multiple users. Use consistent key names for properties across users. */\n    public array $users;\n}\n</code></pre>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/EntityRelationships/run.php</code></p> <p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> field:</p> <pre><code>&lt;?php\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /** @var int[] Correct and complete list of coworker IDs, representing collaboration between users. */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /** @var UserDetail[] Collection of users, correctly capturing the relationships among them. */\n    public array $users;\n}\n</code></pre>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/ChainOfThought/run.php</code></p> <p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>&lt;?php\nclass Role\n{\n    /** Think step by step to determine the correct title. */\n    public string $chainOfThought = '';\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public Role $role;\n}\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/TimeRange/run.php</code></p> <p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public int $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n</code></pre>"},{"location":"concepts/prompting/#adding-context-to-components","title":"Adding Context to Components","text":"<p>Example</p> <p>Run example via CLI: <code>php ./examples/TimeRangeWithCoT/run.php</code></p> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>&lt;?php\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n</code></pre>"},{"location":"concepts/search/","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"concepts/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n</code></pre>"},{"location":"concepts/search/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>Instructor::respond</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nuse Cognesy\\Instructor\\Instructor;\n\nfunction segment(string $data) : Search {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    );\n}\n\nforeach (segment(\"Search for a picture of a cat and a video of a dog\")-&gt;queries as $query) {\n    $query-&gt;execute();\n    // dump($query);\n}\n</code></pre>"},{"location":"hub/","title":"Instructor Hub","text":"<p>Welcome to Instructor Hub. The goal of this section is to provide a set of tutorials and examples to help you get started, and allow you to pull in the code you need to get started with Instructor.</p>"},{"location":"hub/#contributing","title":"Contributing","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single .php file.</li> <li>Please include documentation in the file - check existing examples for the format.</li> <li>Make sure that the code is tested.</li> </ol>"},{"location":"hub/#cli-usage","title":"CLI Usage","text":"<p>Instructor hub comes with a command line interface (CLI) that allows you to view and interact with the tutorials and examples and allows you to pull in the code you need to get started with the API.</p>"},{"location":"hub/#list-cookbooks","title":"List Cookbooks","text":"<p>Run <code>./hub.sh list</code> you can see all the available tutorials and examples.</p> <p><pre><code>$ ./hub.sh list\n</code></pre> ...or on Windows:</p> <pre><code>$ ./hub.bat list\n</code></pre>"},{"location":"hub/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>./hub.sh show {id}</code> to see the full tutorial in the terminal.</p> <pre><code>$ ./hub.sh show {name or id}\n</code></pre> <p>Currently, there is no way to page through the tutorial - feel free to contribute :)</p>"},{"location":"hub/#running-a-cookbook","title":"Running a Cookbook","text":"<p>To run a tutorial, you run <code>./hub.sh run {id}</code> in terminal - it will execute the code and show the output. You need to have your OPENAI_API_KEY set in your environment (.env file in root directory of your copy of instructor-php repo). </p> <pre><code>$ ./hub.sh run {name or id}\n</code></pre>"},{"location":"hub/#running-all-cookbooks","title":"Running all Cookbooks","text":"<p>This is mostly for testing if cookbooks are executed properly, but you can run <code>./hub.sh all {id}</code> to run all the tutorials and examples in the terminal.</p> <pre><code>$ ./hub.sh all {name or id}\n</code></pre>"},{"location":"hub/#call-for-contributions","title":"Call for Contributions","text":"<p>We're looking for a bunch more hub examples, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul> <p>We're also looking for help to catch up with the features available in Instructor Hub for Python (see: https://github.com/jxnl/instructor/blob/main/docs/hub/index.md).</p> <ul> <li> Better viewer with pagination</li> <li> Examples database</li> <li> Pulling in the code to your own dir, so you can get started with the API</li> </ul>"},{"location":"hub/advanced/caching/","title":"Caching","text":"<p>This feature is experimental.</p> <p>You can enable/disable caching for your requests with <code>withCache()</code> method of <code>Instructor</code> class.</p> <p>When caching is enabled, Instructor will store the response in cache and return it on subsequent requests with the same parameters (for given API client).</p> <p>This option is available for all clients. By default, caching is turned off.</p> <p>NOTE: Currently, Instructor does not support caching for streamed responses.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\nuse Cognesy\\Instructor\\Utils\\Profiler\\Profiler;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// OpenAI auth params\n$yourApiKey = Env::get('OPENAI_API_KEY'); // use your own API key\n\n// Create instance of OpenAI client\n$client = (new OpenAIClient($yourApiKey));\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\nProfiler::mark('start');\n\n$user = $instructor-&gt;request(\n    messages: \"Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;get();\n\n$delta = Profiler::mark('no cache')-&gt;mili();\necho \"Time elapsed (no cache, default): $delta msec\\n\\n\";\n\n$user2 = $instructor-&gt;request(\n    messages: \"Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;withCache()-&gt;get();\n\n$delta = Profiler::mark('cache 1st call')-&gt;mili();\necho \"Time elapsed (cache on, 1st call): $delta msec\\n\\n\";\n\n$user3 = $instructor-&gt;request(\n    messages: \"Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;withCache()-&gt;get();\n\n$delta = Profiler::mark('cache 2nd call')-&gt;mili();\necho \"Time elapsed (cache on, 2nd call): $delta msec\\n\\n\";\n\n$user4 = $instructor-&gt;request(\n    messages: \"Jason is 25 years old.\",\n    responseModel: User::class,\n)-&gt;withCache(false)-&gt;get();\n\n$delta = Profiler::mark('cache 3rd call')-&gt;mili();\necho \"Time elapsed (cache turned off again): $delta msec\\n\\n\";\n\n?&gt;\n</code></pre>"},{"location":"hub/advanced/custom_client_parameters/","title":"Customize parameters of OpenAI client","text":"<p>You can provide your own OpenAI client instance to Instructor. This is useful when you want to initialize OpenAI client with custom values - e.g. to call other LLMs which support OpenAI API.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: Env::get('OPENAI_API_KEY'),\n    baseUri: 'https://api.openai.com/v1',\n    connectTimeout: 3,\n    requestTimeout: 30,\n    metadata: ['organization' =&gt; ''],\n);\n\n// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n// Call with custom model and execution mode\n$user = $instructor-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    model: 'gpt-3.5-turbo',\n    mode: Mode::Json,\n);\n\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/advanced/custom_prompts/","title":"Custom prompts","text":"<p>In case you want to take control over the prompts sent by Instructor to LLM for different modes, you can use the <code>prompt</code> parameter in the <code>request()</code> or <code>respond()</code> methods.</p> <p>It will override the default Instructor prompts, allowing you to fully customize how LLM is instructed to process the input.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Instructor;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\n$instructor = (new Instructor)\n    -&gt;onEvent(RequestSentToLLM::class, fn($event)=&gt;dump($event-&gt;request-&gt;body()));\n\nprint(\"\\n# Request for Mode::Tools:\\n\\n\");\n$user = $instructor\n    -&gt;respond(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to extract correct and accurate data from the messages using provided tools.\\n\",\n        mode: Mode::Tools\n    );\n\nprint(\"\\n# Request for Mode::Json:\\n\\n\");\n$user = $instructor\n    -&gt;respond(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with JSON object. Response must follow JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: Mode::Json\n    );\n\nprint(\"\\n# Request for Mode::MdJson:\\n\\n\");\n$user = $instructor\n    -&gt;respond(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        prompt: \"\\nYour task is to respond correctly with strict JSON object containing extracted data within a ```json {} ``` codeblock. Object must validate against this JSONSchema:\\n&lt;|json_schema|&gt;\\n\",\n        mode: Mode::MdJson\n    );\n\n?&gt;\n</code></pre>"},{"location":"hub/advanced/custom_validator/","title":"Custom validation using Symfony Validator","text":"<p>Instructor uses Symfony validation component to validate properties of extracted data. Symfony offers you #[Assert/Callback] annotation to build fully customized validation logic.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\n\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\nuse Symfony\\Component\\Validator\\Context\\ExecutionContextInterface;\n\nclass UserDetails\n{\n    public string $name;\n    public int $age;\n\n    #[Assert\\Callback]\n    public function validateName(ExecutionContextInterface $context, mixed $payload) {\n        if ($this-&gt;name !== strtoupper($this-&gt;name)) {\n            $context-&gt;buildViolation(\"Name must be in all uppercase letters.\")\n                -&gt;atPath('name')\n                -&gt;setInvalidValue($this-&gt;name)\n                -&gt;addViolation();\n        }\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'jason is 25 years old']],\n    responseModel: UserDetails::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;name === \"JASON\");\n?&gt;\n</code></pre>"},{"location":"hub/advanced/data_inputs/","title":"Using structured data as an input","text":"<p>Instructor offers a way to use structured data as an input. This is useful when you want to use object data as input and get another object with a result of LLM inference.</p> <p>The <code>input</code> field of Instructor's <code>respond()</code> and <code>request()</code> methods can be an object, but also an array or just a string.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass Email {\n    public function __construct(\n        public string $address = '',\n        public string $subject = '',\n        public string $body = '',\n    ) {}\n}\n\n$email = new Email(\n    address: 'joe@gmail',\n    subject: 'Status update',\n    body: 'Your account has been updated.'\n);\n\n$translatedEmail = (new Instructor)-&gt;respond(\n    input: $email,\n    responseModel: Email::class,\n    prompt: 'Translate the text fields of email to Spanish. Keep other fields unchanged.',\n);\n\ndump($translatedEmail);\n\nassert($translatedEmail-&gt;address === $email-&gt;address);\nassert($translatedEmail-&gt;subject !== $email-&gt;subject);\nassert($translatedEmail-&gt;body !== $email-&gt;body);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/function_arguments/","title":"Extracting arguments of function or method","text":"<p>Instructor offers FunctionCall class to extract arguments of a function or method from content.</p> <p>This is useful when you want to build tool use capability, e.g. for AI chatbots or agents.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Extras\\FunctionCall\\FunctionCall;\nuse Cognesy\\Instructor\\Instructor;\n\nclass DataStore\n{\n    /** Save user data to storage */\n    public function saveUser(string $name, int $age, string $country) : void {\n        // Save user to database\n        echo \"Saving user ... saveUser('$name', $age, '$country')\\n\";\n    }\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$args = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: FunctionCall::fromMethodName(DataStore::class, 'saveUser'),\n);\n\necho \"\\nCalling the function with the extracted arguments:\\n\";\n(new DataStore)-&gt;saveUser(...$args);\n\necho \"\\nExtracted arguments:\\n\";\ndump($args);\n\nassert(count($args) == 3);\nexpect($args['name'] == 'Jason');\nexpect($args['age'] == 28);\nexpect($args['country'] == 'Germany');\n?&gt;\n</code></pre>"},{"location":"hub/advanced/modules/","title":"Modular processing with Modules","text":"<p>Instructor provides an addon allowing to implement complex processing flows using LLM in a modular way. This addon to Instructor has been inspired by DSPy library for Python (https://github.com/stanfordnlp/dspy).</p> <p>Key components of language program:  - Module subclasses - encapsulate processing logic  - Signatures - define input and output for data processed by modules</p> <p>NOTE: Other concepts from DSPy (optimizer, compiler, evaluator) have not been implemented yet.</p> <p>Module consists of 3 key parts:  - __construct() - initialization of module, prepare dependencies, setup submodules  - signature() - define input and output for data processed by module  - forward() - processing logic, return output data</p> <p><code>Predict</code> class is a special module, that uses Instructor's structured processing capabilities to execute inference on provided inputs and return output in a requested format.</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Extras\\Module\\Addons\\Predict\\Predict;\nuse Cognesy\\Instructor\\Extras\\Module\\Core\\Module;\nuse Cognesy\\Instructor\\Extras\\Module\\Signature\\Attributes\\InputField;\nuse Cognesy\\Instructor\\Extras\\Module\\Signature\\Attributes\\OutputField;\nuse Cognesy\\Instructor\\Extras\\Module\\Signature\\Signature;\nuse Cognesy\\Instructor\\Extras\\Module\\CallData\\SignatureData;\nuse Cognesy\\Instructor\\Instructor;\n\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\n// DATA MODEL DECLARATIONS ////////////////////////////////////////////////////////////////\n\nclass EmailAnalysis extends SignatureData {\n    #[InputField('content of email')]\n    public string $text;\n    #[OutputField('identify most relevant email topic: sales, support, other, spam')]\n    public string $topic;\n    #[OutputField('one word sentiment: positive, neutral, negative')]\n    public string $sentiment;\n\n    public static function for(string $text) : static {\n        return self::fromArgs(text: $text);\n    }\n}\n\nclass CategoryCount {\n    public function __construct(\n        public int $sales = 0,\n        public int $support = 0,\n        public int $spam = 0,\n        public int $other = 0,\n    ) {}\n}\n\nclass EmailStats extends SignatureData {\n    #[InputField('directory containing emails')]\n    public string $directory;\n    #[OutputField('number of emails')]\n    public int $emails;\n    #[OutputField('number of spam emails')]\n    public int $spam;\n    #[OutputField('average sentiment ratio')]\n    public float $sentimentRatio;\n    #[OutputField('spam ratio')]\n    public float $spamRatio;\n    #[OutputField('category counts')]\n    public CategoryCount $categories;\n\n    static public function for(string $directory) : static {\n        return self::fromArgs(directory: $directory);\n    }\n}\n\n// MODULE DECLARATIONS ////////////////////////////////////////////////////////////////////\n\nclass ReadEmails extends Module {\n    public function __construct(\n        private array $directoryContents = []\n    ) {}\n\n    public function signature() : string|Signature {\n        return 'directory -&gt; emails : string[]';\n    }\n\n    protected function forward(string $directory) : array {\n        return $this-&gt;directoryContents[$directory];\n    }\n}\n\nclass ParseEmail extends Module {\n    public function signature() : string|Signature {\n        return 'email -&gt; sender, subject, body';\n    }\n\n    protected function forward(string $email) : array {\n        $parts = explode(',', $email);\n        return [\n            'sender' =&gt; trim(explode(':', $parts[0])[1]),\n            'subject' =&gt; trim(explode(':', $parts[1])[1]),\n            'body' =&gt; trim(explode(':', $parts[2])[1]),\n        ];\n    }\n}\n\nclass GetStats extends Module {\n    public function __construct(\n        private ReadEmails $readEmails,\n        private ParseEmail $parseEmail,\n        private Predict $analyseEmail,\n    ) {}\n\n    public function signature() : string|Signature {\n        return EmailStats::class;\n    }\n\n    protected function forward(string $directory) : EmailStats {\n        $emails = $this-&gt;readEmails-&gt;withArgs(directory: $directory)-&gt;get('emails');\n        $aggregateSentiment = 0;\n        $categories = new CategoryCount;\n        foreach ($emails as $email) {\n            $parsedEmail = $this-&gt;parseEmail-&gt;withArgs(email: $email);\n            $emailData = EmailAnalysis::for(text: $parsedEmail-&gt;get('body'));\n            $emailAnalysis = $this-&gt;analyseEmail-&gt;with($emailData);\n            $topic = $emailAnalysis-&gt;get('topic');\n            $sentiment = $emailAnalysis-&gt;get('sentiment');\n            $topic = (in_array($topic, ['sales', 'support', 'spam'])) ? $topic : 'other';\n            $categories-&gt;$topic++;\n            if ($topic === 'spam') {\n                continue;\n            }\n            $aggregateSentiment += match($sentiment) {\n                'positive' =&gt; 1,\n                'neutral' =&gt; 0,\n                'negative' =&gt; -1,\n            };\n        }\n        $spamRatio = $categories-&gt;spam / count($emails);\n        $sentimentRatio = $aggregateSentiment / (count($emails) - $categories-&gt;spam);\n\n        $result = new EmailStats;\n        $result-&gt;emails = count($emails);\n        $result-&gt;spam = $categories-&gt;spam;\n        $result-&gt;sentimentRatio = $sentimentRatio;\n        $result-&gt;spamRatio = $spamRatio;\n        $result-&gt;categories = $categories;\n        return $result;\n    }\n}\n\n$directoryContents['inbox'] = [\n    'sender: jl@gmail.com, subject: Offer, body: I am happy about the discount you offered and accept contract renewal',\n    'sender: xxx, subject: Free!!!, body: Get Ozempic for free',\n    'sender: joe@wp.pl, subject: Problem, body: My internet connection keeps failing',\n    'sender: paul@x.io, subject: Still no pricing, body: How long do I have to wait for the pricing of custom support service?!?',\n    'sender: joe@wp.pl, subject: Slow connection, body: 2 weeks of waiting and still no improvement of my connection',\n];\n\n// PREPARE DEPENDENCIES\n\n$instructor = (new Instructor);\n$readEmails = new ReadEmails($directoryContents);\n$parseEmail = new ParseEmail();\n$analyseEmail = new Predict(signature: EmailAnalysis::class, instructor: $instructor);\n$getStats = new GetStats($readEmails, $parseEmail, $analyseEmail);\n\n// EXECUTE LANGUAGE PROGRAM\n\n$emailStats = $getStats-&gt;with(EmailStats::for('inbox'));\n\necho \"Results:\\n\";\ndump($emailStats-&gt;get());\n?&gt;\n</code></pre>"},{"location":"hub/advanced/multistep_llm_processing/","title":"Multistep processing with LLMs","text":"<p>Instructor provides an addon allowing to implement complex processing flows using LLM in a modular way. This addon to Instructor has been inspired by DSPy library for Python (https://github.com/stanfordnlp/dspy).</p> <p>This example demonstrates multistep processing with LLMs:  - parse text to extract email data from text (sender, subject and content) -&gt; result is an object containing parsed email data  - fix spelling mistakes in the subject and content fields -&gt; result is an object containing fixed email subject and content  - translate subject into specified language -&gt; result is an object containing translated data</p> <p>All the steps are packaged into a single, reusable module, which is easy to call via:</p> <pre><code>(new ProcessEmail)-&gt;withArgs(\n   text: $text,\n   language: $language,\n);\n</code></pre> <p><code>ProcessEmail</code> inherits from a <code>Module</code>, which is a base class for Instructor modules. It returns a predefined object containing, in this case, the data from all steps of processing.</p> <p>The outputs and flow can be arbitrarily shaped to the needs of specific use case (within the bounds of how Module &amp; Predict components work).</p> <pre><code>&lt;?php\n\nuse Cognesy\\Instructor\\Contracts\\CanProvideSchema;\nuse Cognesy\\Instructor\\Extras\\Module\\Addons\\Predict\\Predict;\nuse Cognesy\\Instructor\\Extras\\Module\\CallData\\Contracts\\HasInputOutputData;\nuse Cognesy\\Instructor\\Extras\\Module\\CallData\\Traits\\AutoSignature;\nuse Cognesy\\Instructor\\Extras\\Module\\Core\\Module;\nuse Cognesy\\Instructor\\Extras\\Module\\Signature\\Attributes\\InputField;\nuse Cognesy\\Instructor\\Extras\\Module\\Signature\\Attributes\\OutputField;\nuse Cognesy\\Instructor\\Extras\\Module\\CallData\\SignatureData;\nuse Cognesy\\Instructor\\Instructor;\n\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\n// DATA MODEL DECLARATIONS ////////////////////////////////////////////////////////////////\n\n//#[Description('extract email details from text')]\nclass ParsedEmail extends SignatureData {\n    // INPUTS\n    #[InputField('text containing email')]\n    public string $text;\n    // OUTPUTS\n    #[OutputField('email address of sender')]\n    public string $senderEmail;\n    #[OutputField('subject of the email')]\n    public string $subject;\n    #[OutputField('body of the email')]\n    public string $body;\n}\n\nclass FixedEmail extends SignatureData {\n    // INPUTS\n    #[InputField('subject of the email')]\n    public string $subject;\n    #[InputField('body of the email')]\n    public string $body;\n    // OUTPUTS\n    #[OutputField('subject of the email with fixed spelling mistakes')]\n    public string $fixedSubject;\n    #[OutputField('body of the email with fixed spelling mistakes')]\n    public string $fixedBody;\n}\n\n// Alternative way to define the class signature data without extending a class\nclass EmailTranslation implements HasInputOutputData, CanProvideSchema {\n    use AutoSignature;\n    // INPUTS\n    #[InputField('subject of email')]\n    public string $subject;\n    #[InputField('body of email')]\n    public string $body;\n    #[InputField('language to translate to')]\n    public string $language;\n    // OUTPUTS\n    #[OutputField('translated subject of email')]\n    public string $translatedSubject;\n    #[OutputField('translated body of email')]\n    public string $translatedBody;\n}\n\nclass Email {\n    public function __construct(\n        public string $senderEmail,\n        public string $subject,\n        public string $body\n    ) {}\n}\n\nclass EmailProcessingResults {\n    public function __construct(\n        public Email $original,\n        public Email $fixed,\n        public Email $translated\n    ) {}\n}\n\n// MODULE DECLARATIONS ////////////////////////////////////////////////////////////////////\n\nclass ProcessEmail extends Module {\n    private Predict $parse;\n    private Predict $fix;\n    private Predict $translate;\n\n    public function __construct() {\n        $instructor = (new Instructor);//-&gt;withClient(new AnthropicClient(Env::get('ANTHROPIC_API_KEY')));//-&gt;wiretap(fn($e) =&gt; $e-&gt;printDump());\n\n        $this-&gt;parse = new Predict(signature: ParsedEmail::class, instructor: $instructor);\n        $this-&gt;fix = new Predict(signature: FixedEmail::class, instructor: $instructor);\n        $this-&gt;translate = new Predict(signature: EmailTranslation::class, instructor: $instructor);\n    }\n\n    public function signature(): string {\n        return 'text: string, language: string -&gt; result: EmailProcessingResults';\n    }\n\n    public function forward(string $text, string $language): EmailProcessingResults {\n        $parsedEmail = $this-&gt;parse-&gt;with(\n            ParsedEmail::fromArgs(\n                text: $text\n            )\n        )-&gt;result();\n\n        $fixedEmail = $this-&gt;fix-&gt;with(\n            FixedEmail::fromArgs(\n                subject: $parsedEmail-&gt;subject,\n                body: $parsedEmail-&gt;body\n            )\n        )-&gt;result();\n\n        $translatedEmail = $this-&gt;translate-&gt;with(\n            EmailTranslation::fromArgs(\n                subject: $fixedEmail-&gt;fixedSubject,\n                body: $fixedEmail-&gt;fixedBody,\n                language: $language\n            )\n        )-&gt;result();\n\n        return new EmailProcessingResults(\n            new Email(\n                $parsedEmail-&gt;senderEmail,\n                $parsedEmail-&gt;subject,\n                $parsedEmail-&gt;body\n            ),\n            new Email(\n                $parsedEmail-&gt;senderEmail,\n                $fixedEmail-&gt;fixedSubject,\n                $fixedEmail-&gt;fixedBody\n            ),\n            new Email(\n                $parsedEmail-&gt;senderEmail,\n                $translatedEmail-&gt;translatedSubject,\n                $translatedEmail-&gt;translatedBody\n            )\n        );\n    }\n}\n\n// EXECUTE LANGUAGE PROGRAM ///////////////////////////////////////////////////////////////\n\n$text = 'sender: jl@gmail.com, subject: Ofer, body: Im hapy abut the discount you offered and accept contrac renewal';\n$language = 'French';\n\n$result = (new ProcessEmail)-&gt;withArgs(text: $text, language: $language)-&gt;result();\n\necho \"Results:\\n\";\ndump($result);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/partial_updates/","title":"Streaming partial updates during inference","text":"<p>Instructor can process LLM's streamed responses to provide partial updates that you can use to update the model with new data as the response is being generated. You can use it to improve user experience by updating the UI with partial data before the full response is received.</p> <p><pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    echo chr(27).chr(91).'H'.chr(27).chr(91).'J';\n\n    // Print explanation\n    echo \"Waiting 250ms on every update received to make changes easier to observe...\\n\";\n\n    // Display the partial object\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$user = (new Instructor)-&gt;request(\n    messages: $text,\n    responseModel: UserDetail::class,\n    options: ['stream' =&gt; true]\n)-&gt;onPartialUpdate(partialUpdate(...))-&gt;get();\n\necho \"All tokens received, fully completed object available in `\\$user` variable.\\n\";\necho '$user = '.\"\\n\";\ndump($user);\n\nassert(!empty($user-&gt;roles));\nassert(!empty($user-&gt;hobbies));\nassert($user-&gt;location == 'San Francisco');\nassert($user-&gt;age == 25);\nassert($user-&gt;name == 'Jason');\n?&gt;\n</code></pre>"},{"location":"hub/advanced/providing_examples/","title":"Providing example inputs and outputs","text":"<p>To improve the results of LLM inference you can provide examples of the expected output. This will help LLM to understand the context and the expected structure of the output.</p> <p>It is typically useful in the <code>Mode::Json</code> and <code>Mode::MdJson</code> modes, where the output is expected to be a JSON object.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Data\\Example;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\necho \"\\nREQUEST:\\n\";\n$user = (new Instructor)\n    -&gt;onEvent(RequestSentToLLM::class, fn($event)=&gt;dump($event-&gt;request-&gt;body()))\n    -&gt;request(\n        messages: \"Our user Jason is 25 years old.\",\n        responseModel: User::class,\n        examples: [\n            new Example(\n                input: \"John is 50 and works as a teacher.\",\n                output: ['name' =&gt; 'John', 'age' =&gt; 50]\n            ),\n            new Example(\n                input: \"We have recently hired Ian, who is 27 years old.\",\n                output: ['name' =&gt; 'Ian', 'age' =&gt; 27],\n                template: \"example input:\\n{input}\\noutput:\\n```json\\n{output}\\n```\\n\",\n            ),\n        ],\n        mode: Mode::Json)\n    -&gt;get();\n\necho \"\\nOUTPUT:\\n\";\ndump($user);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/scalars/","title":"Extracting scalar values","text":"<p>Sometimes we just want to get quick results without defining a class for the response model, especially if we're trying to get a straight, simple answer in a form of string, integer, boolean or float. Instructor provides a simplified API for such cases.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Scalar\\Scalar;\nuse Cognesy\\Instructor\\Instructor;\n\nenum CitizenshipGroup : string {\n    case US = \"us\";\n    case Canada = \"uk\";\n    case Other = \"other\";\n}\n\n$text = \"His name is Jason, he is 28 years old and he lives in Germany.\";\n$value = (new Instructor)-&gt;respond(\n    messages: [\n        ['role' =&gt; 'system', 'content' =&gt; $text],\n        ['role' =&gt; 'user', 'content' =&gt; 'What is Jason\\'s citizenship?'],\n    ],\n    responseModel: Scalar::enum(CitizenshipGroup::class, name: 'citizenshipGroup'),\n);\n\n\ndump($value);\n\nassert($value instanceof CitizenshipGroup);\nexpect($value == CitizenshipGroup::Other);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/sequences/","title":"Extracting sequences of objects","text":"<p>Sequences are a special type of response model that can be used to represent a list of objects.</p> <p>It is usually more convenient not create a dedicated class with a single array property just to handle a list of objects of a given class.</p> <p>Additional, unique feature of sequences is that they can be streamed per each completed item in a sequence, rather than on any property update.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\Instructor;\n\nclass Person\n{\n    public string $name;\n    public int $age;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. Jane is 18 yo. John is 30 years old. Anna is 2 years younger than him.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"OUTPUT:\\n\");\n$list = (new Instructor)\n    -&gt;request(\n        messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n        responseModel: Sequence::of(Person::class),\n        options: ['stream' =&gt; true],\n    )\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; dump($sequence-&gt;last()))\n    -&gt;get();\n\n\ndump(count($list));\n\nassert(count($list) === 4);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/streaming/","title":"Streaming","text":"<p>Instructor can process LLM's streamed responses to provide partial response model updates that you can use to update the model with new data as the response is being generated.</p> <p><pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserRole\n{\n    /** Monotonically increasing identifier */\n    public int $id;\n    public string $title = '';\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public string $location;\n    /** @var UserRole[] */\n    public array $roles;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// This function will be called every time a new token is received\nfunction partialUpdate($partial) {\n    // Clear the screen and move the cursor to the top\n    echo chr(27).chr(91).'H'.chr(27).chr(91).'J';\n\n    // Print explanation\n    echo \"Waiting 250ms on every update received to make changes easier to observe...\\n\";\n\n    // Display the partial object\n\n    dump($partial);\n\n    // Wait a bit before clearing the screen to make partial changes slower.\n    // Don't use this in your application :)\n    usleep(250000);\n}\n?&gt;\n</code></pre> Now we can use this data model to extract arbitrary properties from a text message. As the tokens are streamed from LLM API, the <code>partialUpdate</code> function will be called with partially updated object of type <code>UserDetail</code> that you can use, usually to update the UI.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old, he is an engineer and tech lead. He lives in\n    San Francisco. He likes to play soccer and climb mountains.\n    TEXT;\n\n$stream = (new Instructor)-&gt;request(\n    messages: $text,\n    responseModel: UserDetail::class,\n    options: ['stream' =&gt; true]\n)-&gt;stream();\n\nforeach ($stream-&gt;partials() as $partial) {\n    partialUpdate($partial);\n}\n\n$user = $stream-&gt;getLastUpdate();\n\nassert($user-&gt;name === 'Jason');\nassert($user-&gt;age === 25);\n?&gt;\n</code></pre>"},{"location":"hub/advanced/structures/","title":"Structures","text":"<p>Structures allow dynamically define the shape of data to be extracted by LLM, e.g. during runtime.</p> <p>Use <code>Structure::define()</code> to define the structure and pass it to Instructor as response model.</p> <p>If <code>Structure</code> instance has been provided as a response model, Instructor returns an array in the shape you defined.</p> <p>See more: Structures</p> <pre><code>&lt;?php\nini_set('display_errors', 1);\nini_set('display_startup_errors', 1);\nerror_reporting(E_ALL);\n\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Structure\\Field;\nuse Cognesy\\Instructor\\Extras\\Structure\\Structure;\nuse Cognesy\\Instructor\\Instructor;\n\nenum Role : string {\n    case Manager = 'manager';\n    case Line = 'line';\n}\n\n$structure = Structure::define('person', [\n    Field::string('name','Name of the person'),\n    Field::int('age', 'Age of the person')-&gt;validIf(\n        fn($value) =&gt; $value &gt; 0, \"Age has to be positive number\"\n    ),\n    Field::structure('address', [\n        Field::string('street', 'Street name')-&gt;optional(),\n        Field::string('city', 'City name'),\n        Field::string('zip', 'Zip code')-&gt;optional(),\n    ], 'Address of the person'),\n    Field::enum('role', Role::class, 'Role of the person'),\n], 'A person object');\n\n$text = &lt;&lt;&lt;TEXT\n    Jane Doe lives in Springfield, 50210. She is 25 years old and works as manager at McDonald's.\n    McDonald's in Ney York is located at 456 Elm St, NYC, 12345.\n    TEXT;\n\nprint(\"INPUT:\\n$text\\n\\n\");\n$person = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: $structure,\n);\n\nprint(\"OUTPUT:\\n\");\nprint(\"Name: \" . $person-&gt;name . \"\\n\");\nprint(\"Age: \" . $person-&gt;age . \"\\n\");\nprint(\"Address / city: \" . $person-&gt;address-&gt;city . \"\\n\");\nprint(\"Address / ZIP: \" . $person-&gt;address-&gt;zip . \"\\n\");\nprint(\"Role: \" . $person-&gt;role-&gt;value . \"\\n\");\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_anthropic/","title":"Support for Anthropic API","text":"<p>Instructor supports Anthropic API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility: - Mode::MdJson, Mode::Json - supported - Mode::Tools - not supported yet</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Anthropic\\AnthropicClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Create instance of client initialized with custom parameters\n$client = new AnthropicClient(\n    apiKey: Env::get('ANTHROPIC_API_KEY'),\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'claude-3-haiku-20240307',\n    mode: Mode::Tools,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n);\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_anyscale/","title":"Support for Anyscale API","text":"<p>Anyscale is hosted language model provider that offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Anyscale as demonstrated below.</p> <p>Please note that some models support Mode::Tools or Mode::Json, which are much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Anyscale\\AnyscaleClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Events\\PartialsGenerator\\StreamedResponseReceived;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('ANYSCALE_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new AnyscaleClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n        mode: Mode::Json,\n        //options: ['stream' =&gt; true],\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_azure_oai/","title":"Support for Azure OpenAI API","text":"<p>You can connect to Azure OpenAI instance using a dedicated client provided by Instructor. Please note it requires setting up your own model deployment using Azure OpenAI service console.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Azure\\AzureClient;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n/// Custom client parameters: base URI\n$resourceName = Env::get('AZURE_OPENAI_RESOURCE_NAME'); // set your own value/source\n\n$client = (new AzureClient(\n    apiKey: Env::get('AZURE_OPENAI_API_KEY'),\n    resourceName: 'instructor-dev', // set your own value/source\n    deploymentId: 'gpt-35-turbo-16k', // set your own value/source\n    apiVersion: '2024-02-01',\n));\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n// Call with your model name and preferred execution mode\n$user = $instructor-&gt;respond(\n    messages: \"Our user Jason is 25 years old.\",\n    responseModel: User::class,\n    model: 'gpt-35-turbo-16k', // set your own value/source\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_cohere/","title":"Support for Cohere API","text":"<p>Instructor supports Cohere API - you can find the details on how to configure the client in the example below.</p> <p>Mode compatibility:  - Mode::MdJson - supported, recommended  - Mode::Json - not supported by Cohere  - Mode::Tools - partially supported, not recommended</p> <p>Reasons Mode::Tools is not recommended:</p> <ul> <li>Cohere does not support JSON Schema, which only allows to extract very simple data schemas.</li> <li>Performance of the currently available versions of Cohere models in tools mode for Instructor use case (data extraction) is extremely poor.</li> </ul> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Cohere\\CohereClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Create instance of client initialized with custom parameters\n$client = new CohereClient(\n    apiKey: Env::get('COHERE_API_KEY'),\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'command-r-plus',\n    mode: Mode::MdJson,\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n);\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_fireworks_ai/","title":"Support for Fireworks.ai API","text":"<p>Please note that the larger Mistral models support Mode::Json, which is much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\FireworksAI\\FireworksAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('FIREWORKSAI_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new FireworksAIClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'accounts/fireworks/models/mixtral-8x7b-instruct',\n        mode: Mode::Json,\n    //options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_gemini/","title":"Support for Google Gemini API","text":"<p>Google offers Gemini models which perform well in bechmarks. Here's how you can use Instructor with Gemini API.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Gemini\\GeminiClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('GEMINI_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new GeminiClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'gemini-1.5-flash',\n        //options: ['stream' =&gt; true],\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        mode: Mode::MdJson,\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_groq/","title":"Support for Groq API","text":"<p>Groq is LLM providers offering a very fast inference thanks to their custom hardware. They provide a several models - Llama2, Mixtral and Gemma. Here's how you can use Instructor with Groq API.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Groq\\GroqClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public ?int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('GROQ_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new GroqClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'llama3-8b-8192',\n        options: ['stream' =&gt; false],\n        examples: [[\n            'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n            'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n        ]],\n        mode: Mode::Json,\n    );\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_mistral/","title":"Support for Mistral API","text":"<p>Mistral.ai is a company that builds OS language models, but also offers a platform hosting those models. You can use Instructor with Mistral API by configuring the client as demonstrated below.</p> <p>Please note that the larger Mistral models support Mode::Json, which is much more reliable than Mode::MdJson.</p> <p>Mode compatibility:  - Mode::Tools - Mistral-Small / Mistral-Medium / Mistral-Large  - Mode::Json - Mistral-Small / Mistral-Medium / Mistral-Large  - Mode::MdJson - Mistral 7B / Mixtral 8x7B</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Mistral\\MistralClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('MISTRAL_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new MistralClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.mistral.ai/v1',\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'open-mixtral-8x7b',\n        mode: Mode::Json,\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_ollama/","title":"Support for local Ollama","text":"<p>You can use Instructor with local Ollama instance. Please note that, at least currently, OS models do not perform on par with OpenAI (GPT-3.5 or GPT-4) model.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Ollama\\OllamaClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Events\\Request\\ResponseReceivedFromLLM;\nuse Cognesy\\Instructor\\Instructor;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Create instance of Ollama client with default settings\n$client = new OllamaClient();\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n// Listen to events to print request/response data\n$instructor-&gt;onEvent(RequestSentToLLM::class, function($event) {\n    print(\"Request sent to LLM:\\n\\n\");\n\n    dump($event-&gt;request);\n});\n$instructor-&gt;onEvent(ResponseReceivedFromLLM::class, function($event) {\n    print(\"Received response from LLM:\\n\\n\");\n\n    dump($event-&gt;response);\n});\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'llama2:latest',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n    mode: Mode::Json,\n);\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_open_ai/","title":"Support for OpenAI API","text":"<p>This is the default client used by Instructor.</p> <p>Mode compatibility:  - Mode::Tools (recommended)  - Mode::Json  - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// OpenAI auth params\n$yourApiKey = Env::get('OPENAI_API_KEY'); // use your own API key\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenAIClient(\n    apiKey: $yourApiKey,\n    baseUri: 'https://api.openai.com/v1',\n    organization: '',\n    connectTimeout: 3,\n    requestTimeout: 30,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'gpt-3.5-turbo',\n    examples: [[\n        'input' =&gt; 'Ive got email Frank - their developer. He asked to come back to him frank@hk.ch. Btw, he plays on drums!',\n        'output' =&gt; ['age' =&gt; null, 'name' =&gt; 'Frank', 'role' =&gt; 'developer', 'hobbies' =&gt; ['playing drums'],],\n    ]],\n);\n\nprint(\"Completed response model:\\n\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_open_router/","title":"Support for OpenRouter API","text":"<p>You can use Instructor with OpenRouter API. OpenRouter provides easy, unified access to multiple open source and commercial models. Read OpenRouter docs to learn more about the models they support.</p> <p>Please note that OS models are in general weaker than OpenAI ones, which may result in lower quality of responses or extraction errors. You can mitigate this (partially) by using validation and <code>maxRetries</code> option to make Instructor automatically reattempt the extraction in case of extraction issues.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenRouter\\OpenRouterClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// OpenRouter client params\n$yourApiKey = Env::get('OPENROUTER_API_KEY'); // or your own value/source\n\n// Create instance of OpenAI client initialized with custom parameters\n$client = new OpenRouterClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor-&gt;respond(\n    messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n    responseModel: User::class,\n    model: 'mistralai/mixtral-8x7b-instruct:nitro',\n    mode: Mode::Json,\n    //options: ['stream' =&gt; true ]\n);\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/api_support/llm_support_together_ai/","title":"Support for Together.ai API","text":"<p>Together.ai hosts a number of language models and offers inference API with support for chat completion, JSON completion, and tools call. You can use Instructor with Together.ai as demonstrated below.</p> <p>Please note that some Together.ai models support Mode::Tools or Mode::Json, which are much more reliable than Mode::MdJson.</p> <p>Mode compatibility: - Mode::Tools - selected models - Mode::Json - selected models - Mode::MdJson</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\TogetherAI\\TogetherAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\nenum UserType : string {\n    case Guest = 'guest';\n    case User = 'user';\n    case Admin = 'admin';\n}\n\nclass User {\n    public int $age;\n    public string $name;\n    public string $username;\n    public UserType $role;\n    /** @var string[] */\n    public array $hobbies;\n}\n\n// Mistral instance params\n$yourApiKey = Env::get('TOGETHERAI_API_KEY'); // set your own API key\n\n// Create instance of client initialized with custom parameters\n$client = new TogetherAIClient(\n    apiKey: $yourApiKey,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\n$user = $instructor\n    -&gt;respond(\n        messages: \"Jason (@jxnlco) is 25 years old and is the admin of this project. He likes playing football and reading books.\",\n        responseModel: User::class,\n        model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n        mode: Mode::Json,\n        //options: ['stream' =&gt; true ]\n    );\n\nprint(\"Completed response model:\\n\\n\");\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/basics/basic/","title":"Basic use","text":"<p>Instructor allows you to use large language models to extract information from the text (or content of chat messages), while following the structure you define.</p> <p>LLM does not 'parse' the text to find and retrieve the information. Extraction leverages LLM ability to comprehend provided text and infer the meaning of the information it contains to fill fields of the response object with values that match the types and semantics of the class fields.</p> <p>The simplest way to use the Instructor is to call the <code>respond</code> method on the Instructor instance. This method takes a string (or an array of strings in the format of OpenAI chat messages) as input and returns a data extracted from provided text (or chat) using the LLM inference.</p> <p>Returned object will contain the values of fields extracted from the text.</p> <p>The format of the extracted data is defined by the response model, which in this case is a simple PHP class with some public properties.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\nclass User {\n    public int $age;\n    public string $name;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = \"Jason is 25 years old and works as an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: User::class,\n);\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/basics/basic_via_mixin/","title":"Basic use with HandlesSelfExtraction trait","text":"<p>Instructor provides <code>HandlesSelfExtraction</code> trait that you can use to enable extraction capabilities directly on class via static <code>extract()</code> method.</p> <p><code>extract()</code> method returns an instance of the class with the data extracted using the Instructor.</p> <p><code>extract()</code> method has following signature (you can also find it in the <code>CanSelfExtract</code> interface):</p> <pre><code>static public function extract(\n    string|array $messages, // (required) The message(s) to extract data from\n    string $model = '',     // (optional) The model to use for extraction (otherwise - use default)\n    int $maxRetries = 2,    // (optional) The number of retries in case of validation failure\n    array $options = [],    // (optional) Additional data to pass to the Instructor or LLM API\n    array $examples = [],   // (optional) Examples to include in the prompt\n    string $toolName = '',  // (optional) The name of the tool call - used to add semantic information for LLM\n    string $toolDescription = '', // (optional) The description of the tool call - as above\n    string $prompt = '',    // (optional) The prompt to use for extraction\n    string $retryPrompt = '', // (optional) The prompt to use in case of validation failure\n    Mode $mode = Mode::Tools, // (optional) The mode to use for extraction\n    Instructor $instructor = null // (optional) The Instructor instance to use for extraction\n) : static;\n</code></pre> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Mixin\\HandlesSelfExtraction;\n\nclass User {\n    use HandlesSelfExtraction;\n\n    public int $age;\n    public string $name;\n}\n\n$user = User::extract(\"Jason is 25 years old and works as an engineer.\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/basics/complex_extraction/","title":"Extraction of complex objects","text":"<p>This is an example of extraction of a very complex structure from the provided text.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\OpenAI\\OpenAIClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n$client = new OpenAIClient(\n    apiKey: Env::get('OPENAI_API_KEY'),\n    requestTimeout: 90,\n);\n\n$instructor = (new Instructor)-&gt;withClient($client);\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $instructor\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;request(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        mode: Mode::Json,\n        options: [\n            'max_tokens' =&gt; 2048,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"hub/basics/complex_extraction_claude/","title":"Extraction of complex objects (Anthropic)","text":"<p>This is an example of extraction of a very complex structure from the provided text with Anthropic Claude 3 model.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Anthropic\\AnthropicClient;\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Extras\\Sequence\\Sequence;\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Utils\\Env;\n\n$report = &lt;&lt;&lt;'EOT'\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\necho \"Extracting project events from the report:\\n\\n\";\necho $report . \"\\n\\n\";\n\nclass ProjectEvents {\n    /**\n     * List of events extracted from the text\n     * @var ProjectEvent[]\n     */\n    public array $events = [];\n}\n\n/** Represents a project event */\nclass ProjectEvent {\n    /** Title of the event - this should be a short, descriptive title of the event */\n    public string $title = '';\n    /** Concise, informative description of the event */\n    public string $description = '';\n    /** Type of the event */\n    public ProjectEventType $type = ProjectEventType::Other;\n    /** Status of the event */\n    public ProjectEventStatus $status = ProjectEventStatus::Unknown;\n    /** Stakeholders involved in the event */\n    /** @var Stakeholder[] */\n    public array $stakeholders = [];\n    /** Date of the event if reported in the text */\n    public ?string $date = '';\n}\n\n/** Represents status of project event */\nenum ProjectEventStatus: string {\n    case Open = 'open';\n    case Closed = 'closed';\n    case Unknown = 'unknown';\n}\n\n/** Represents type of project event */\nenum ProjectEventType: string {\n    case Risk = 'risk';\n    case Issue = 'issue';\n    case Action = 'action';\n    case Progress = 'progress';\n    case Other = 'other';\n}\n\n/** Represents a project stakeholder */\nclass Stakeholder {\n    /** Name of the stakeholder */\n    public string $name = '';\n    /** Role of the stakeholder, if specified */\n    public StakeholderRole $role = StakeholderRole::Other;\n    /** Any details on the stakeholder, if specified - any mentions of company, organization, structure, group, team, function */\n    public ?string $details = '';\n}\n\nenum StakeholderRole: string {\n    case Customer = 'customer';\n    case Vendor = 'vendor';\n    case SystemIntegrator = 'system integrator';\n    case Other = 'other';\n}\n\n// Create instance of client initialized with custom parameters\n$client = new AnthropicClient(\n    apiKey: Env::get('ANTHROPIC_API_KEY'),\n    requestTimeout: 90,\n);\n\n/// Get Instructor with the default client component overridden with your own\n$instructor = (new Instructor)-&gt;withClient($client);\n\necho \"PROJECT EVENTS:\\n\\n\";\n\n$events = $instructor\n    -&gt;onSequenceUpdate(fn($sequence) =&gt; displayEvent($sequence-&gt;last()))\n    -&gt;request(\n        messages: $report,\n        responseModel: Sequence::of(ProjectEvent::class),\n        model: 'claude-3-haiku-20240307', //'claude-3-sonnet-20240229',\n        prompt: 'Extract a list of project events with all the details from the report below in JSON format using schema: &lt;|json_schema|&gt;',\n        mode: Mode::MdJson,\n        examples: [['input' =&gt; 'Acme Insurance project to implement SalesTech CRM solution is currently in RED status due to delayed delivery of document production system, led by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution with the vendor. Production deployment plan has been finalized on Aug 15th and awaiting customer approval.', 'output' =&gt; [[\"type\" =&gt; \"object\", \"title\" =&gt; \"sequenceOfProjectEvent\", \"description\" =&gt; \"A sequence of ProjectEvent\", \"properties\" =&gt; [\"list\" =&gt; [[\"title\" =&gt; \"Absorbing delay by deploying extra resources\", \"description\" =&gt; \"System integrator (SysCorp) are working to absorb some of the delay by deploying extra resources to speed up development when the doc production is done.\", \"type\" =&gt; \"action\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"SysCorp\", \"role\" =&gt; \"system integrator\", \"details\" =&gt; \"System integrator\",],], \"date\" =&gt; \"2021-09-01\",], [\"title\" =&gt; \"Finalization of production deployment plan\", \"description\" =&gt; \"Production deployment plan has been finalized on Aug 15th and awaiting customer approval.\", \"type\" =&gt; \"progress\", \"status\" =&gt; \"open\", \"stakeholders\" =&gt; [[\"name\" =&gt; \"Acme\", \"role\" =&gt; \"customer\", \"details\" =&gt; \"Customer\",],], \"date\" =&gt; \"2021-08-15\",],],]]]]],\n        options: [\n            'max_tokens' =&gt; 4096,\n            'stream' =&gt; true,\n        ])\n    -&gt;get();\n\necho \"TOTAL EVENTS: \" . count($events) . \"\\n\";\n\nfunction displayEvent(ProjectEvent $event) : void {\n    echo \"Event: {$event-&gt;title}\\n\";\n    echo \" - Descriptions: {$event-&gt;description}\\n\";\n    echo \" - Type: {$event-&gt;type-&gt;value}\\n\";\n    echo \" - Status: {$event-&gt;status-&gt;value}\\n\";\n    echo \" - Date: {$event-&gt;date}\\n\";\n    if (empty($event-&gt;stakeholders)) {\n        echo \" - Stakeholders: none\\n\";\n    } else {\n        echo \" - Stakeholders:\\n\";\n        foreach($event-&gt;stakeholders as $stakeholder) {\n            echo \"   - {$stakeholder-&gt;name} ({$stakeholder-&gt;role-&gt;value})\\n\";\n        }\n    }\n    echo \"\\n\";\n}\n?&gt;\n</code></pre>"},{"location":"hub/basics/maybe/","title":"Handling errors with <code>Maybe</code> helper class","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Maybe\\Maybe;\nuse Cognesy\\Instructor\\Instructor;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n\n$text = 'We don\\'t know anything about this guy.';\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class)\n);\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === false);\nassert(!empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() === null);\n\n$text = \"Jason is our new developer, he is 25 years old.\";\necho \"\\nINPUT:\\n$text\\n\";\n\n$maybeUser = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Maybe::is(User::class)\n);\n\necho \"\\nOUTPUT:\\n\";\n\ndump($maybeUser-&gt;get());\n\nassert($maybeUser-&gt;hasValue() === true);\nassert(empty($maybeUser-&gt;error()));\nassert($maybeUser-&gt;get() != null);\nassert($maybeUser-&gt;get() instanceof User);\n?&gt;\n</code></pre>"},{"location":"hub/basics/optional_fields/","title":"Making some fields optional","text":"<p>Use PHP's nullable types by prefixing type name with question mark (?) to mark component fields which are optional and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserRole\n{\n    public string $title;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    public ?UserRole $role;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Jason is 25 years old.\"]],\n    responseModel: UserDetail::class,\n);\n\n\ndump($user);\n\nassert(!isset($user-&gt;role));\n?&gt;\n</code></pre>"},{"location":"hub/basics/private_vs_public_fields/","title":"Private vs public object field","text":"<p>Instructor only sets public fields of the object with the data provided by LLM. Private and protected fields are left unchanged. If you want to access them directly after extraction, consider providing default values for them.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass User\n{\n    public string $name;\n    public int $age;\n    public string $password = '';\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\nclass UserWithPrivateField\n{\n    public string $name;\n    private int $age = 0;\n    private string $password = '';\n\n    public function getAge(): int {\n        return $this-&gt;age;\n    }\n\n    public function getPassword(): string {\n        return $this-&gt;password;\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. His password is '123admin'.\n    TEXT;\n\n\n// CASE 1: Class with public fields\n\n$user = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: User::class\n);\n\necho \"User with public fields\\n\";\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;getAge() === 25);\nassert($user-&gt;getPassword() === '123admin');\n\n\n// CASE 2: Class with some private fields\n\n$userPriv = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: UserWithPrivateField::class,\n);\n\necho \"User with private 'password' and 'age' fields\\n\";\n\ndump($userPriv);\n\nassert($userPriv-&gt;name === \"Jason\");\nassert($userPriv-&gt;getAge() === 0);\nassert($userPriv-&gt;getPassword() === '');\n?&gt;\n</code></pre>"},{"location":"hub/basics/self_correction/","title":"Automatic correction based on validation results","text":"<p>Instructor uses validation errors to inform LLM on the problems identified in the response, so that LLM can try self-correcting in the next attempt.</p> <p>In case maxRetries parameter is provided and LLM response does not meet validation criteria, Instructor will make subsequent inference attempts until results meet the requirements or maxRetries is reached.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidated;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationAttempt;\nuse Cognesy\\Instructor\\Events\\Response\\ResponseValidationFailed;\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    public string $email;\n}\n$text = \"you can reply to me via jason@gmailcom -- Jason\";\n\nprint(\"INPUT:\\n$text\\n\\n\");\n\nprint(\"RESULTS:\\n\");\n$user = (new Instructor)\n    -&gt;onEvent(RequestSentToLLM::class, fn($event) =&gt; print(\"[ ] Requesting LLM response...\\n\"))\n    -&gt;onEvent(ResponseValidationAttempt::class, fn($event) =&gt; print(\"[?] Validating:\\n    \".json_encode($event-&gt;response).\"\\n\"))\n    -&gt;onEvent(ResponseValidationFailed::class, fn($event) =&gt; print(\"[!] Validation failed:\\n    $event\\n\"))\n    -&gt;onEvent(ResponseValidated::class, fn($event) =&gt; print(\"[ ] Validation succeeded.\\n\"))\n    -&gt;respond(\n        messages: $text,\n        responseModel: UserDetails::class,\n        maxRetries: 3\n    );\n\nprint(\"\\nOUTPUT:\\n\");\n\ndump($user);\n\nassert($user-&gt;email === \"jason@gmail.com\");\n?&gt;\n</code></pre>"},{"location":"hub/basics/using_attributes/","title":"Using attributes","text":"<p>Instructor supports <code>Description</code> and <code>Instructions</code> attributes to provide more context to the language model or to provide additional instructions to the model.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Schema\\Attributes\\Description;\nuse Cognesy\\Instructor\\Schema\\Attributes\\Instructions;\n\n// Step 1: Define a class that represents the structure and semantics\n// of the data you want to extract\n#[Description(\"Information about user\")]\nclass User {\n    #[Description(\"User's age\")]\n    public int $age;\n    #[Instructions(\"Make it ALL CAPS\")]\n    public string $name;\n    #[Description(\"User's job\")]\n    #[Instructions(\"Ignore hobbies, identify profession\")]\n    public string $job;\n}\n\n// Step 2: Get the text (or chat messages) you want to extract data from\n$text = \"Jason is 25 years old, 10K runner, speaker and an engineer.\";\nprint(\"Input text:\\n\");\nprint($text . \"\\n\\n\");\n\n// Step 3: Extract structured data using default language model API (OpenAI)\nprint(\"Extracting structured data using LLM...\\n\\n\");\n$user = (new Instructor)-&gt;respond(\n    messages: $text,\n    responseModel: User::class,\n);\n\n// Step 4: Now you can use the extracted data in your application\nprint(\"Extracted data:\\n\");\n\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\nassert(isset($user-&gt;job));\n?&gt;\n</code></pre>"},{"location":"hub/basics/validation/","title":"Validation","text":"<p>Instructor uses validation to verify if the response generated by LLM meets the requirements of your response model. If the response does not meet the requirements, Instructor will throw an exception.</p> <p>Instructor uses Symfony's Validator component to validate the response, check their documentation for more information on the usage: https://symfony.com/doc/current/components/validator.html</p> <p>Following example demonstrates how to use Symfony Validator's constraints to validate the email field of response.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\nuse Symfony\\Component\\Validator\\Constraints as Assert;\n\nclass UserDetails\n{\n    public string $name;\n    #[Assert\\Email]\n    #[Assert\\NotBlank]\n    /** Find user's email provided in the text */\n    public string $email;\n}\n\n$caughtException = false;\n$user = (new Instructor)-&gt;request(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"you can reply to me via mail -- Jason\"]],\n    responseModel: UserDetails::class,\n)-&gt;onError(function($e) use (&amp;$caughtException) {\n    $caughtException = true;\n})-&gt;get();\n\n\ndump($user);\n\nassert($user === null);\nassert($caughtException === true);\n?&gt;\n</code></pre>"},{"location":"hub/basics/validation_mixin/","title":"Validation across multiple fields","text":"<p>Sometimes property level validation is not enough - you may want to check values of multiple properties and based on the combination of them decide to accept or reject the response. Or the assertions provided by Symfony may not be enough for your use case.</p> <p>In such case you can easily add custom validation code to your response model by: - using <code>ValidationMixin</code> - and defining validation logic in <code>validate()</code> method.</p> <p>In this example LLM should be able to correct typo in the message (graduation year we provided is <code>1010</code> instead of <code>2010</code>) and respond with correct graduation year.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass UserDetails\n{\n    use ValidationMixin;\n\n    public string $name;\n    public int $birthYear;\n    public int $graduationYear;\n\n    public function validate() : ValidationResult {\n        if ($this-&gt;graduationYear &gt; $this-&gt;birthYear) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'graduationYear',\n            value: $this-&gt;graduationYear,\n            message: \"Graduation year has to be bigger than birth year.\"\n        );\n    }\n}\n\n$user = (new Instructor)\n    -&gt;respond(\n        messages: [['role' =&gt; 'user', 'content' =&gt; 'Jason was born in 2000 and graduated in 1023.']],\n        responseModel: UserDetails::class,\n        model: 'gpt-4o',\n        maxRetries: 2,\n    );\n\n\ndump($user);\n\nassert($user-&gt;graduationYear === 2023);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/arbitrary_properties/","title":"Arbitrary properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\n\nclass Property\n{\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    public int $age;\n    public string $name;\n    /** @var Property[] Extract any other properties that might be relevant */\n    public array $properties;\n}\n?&gt;\n</code></pre> <p>Now we can use this data model to extract arbitrary properties from a text message in a form that is easier for future processing.</p> <pre><code>&lt;?php\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives\n    in a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    mode: Mode::Json,\n);\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(!empty($user-&gt;properties));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/arbitrary_properties_consistency/","title":"Consistent values of arbitrary properties","text":"<p>For multiple records containing arbitrary properties, instruct LLM to get more consistent key names when extracting properties.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    public int $id;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetails\n{\n    /**\n     * @var UserDetail[] Extract information for multiple users.\n     * Use consistent key names for properties across users.\n     */\n    public array $users;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a Python programmer. Amanda is UX designer.\n    John is 40yo and he's CEO.\n    TEXT;\n\n$list = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetails::class,\n);\n\ndump($list);\n\nassert(!empty($list-&gt;users));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/chain_of_summaries/","title":"Chain of Summaries","text":"<p>This is an example of summarization with increasing amount of details. Instructor is provided with data structure containing instructions on how to create increasingly detailed summaries of the project report.</p> <p>It starts with generating an overview of the project, followed by X iterations of increasingly detailed summaries. Each iteration should contain all the information from the previous summary, plus a few additional facts from the content which are most relevant and missing from the previous iteration.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n$report = &lt;&lt;&lt;EOT\n    [2021-09-01]\n    Acme Insurance project to implement SalesTech CRM solution is currently\n    in RED status due to delayed delivery of document production system, led\n    by 3rd party vendor - Alfatech. Customer (Acme) is discussing the resolution\n    with the vendor. Due to dependencies it will result in delay of the\n    ecommerce track by 2 sprints. System integrator (SysCorp) are working\n    to absorb some of the delay by deploying extra resources to speed up\n    development when the doc production is done. Another issue is that the\n    customer is not able to provide the test data for the ecommerce track.\n    SysCorp notified it will impact stabilization schedule unless resolved by\n    the end of the month. Steerco has been informed last week about the\n    potential impact of the issues, but insists on maintaining release schedule\n    due to marketing campaign already ongoing. Customer executives are asking\n    us - SalesTech team - to confirm SysCorp's assessment of the situation.\n    We're struggling with that due to communication issues - SysCorp team has\n    not shown up on 2 recent calls. Lack of insight has been escalated to\n    SysCorp's leadership team yesterday, but we've got no response yet. The\n    previously reported Integration Proxy connectivity issue which was blocking\n    policy track has been resolved on 2021-08-30 - the track is now GREEN.\n    Production deployment plan has been finalized on Aug 15th and awaiting\n    customer approval.\n    EOT;\n\n/** Executive level summary of the project */\nclass Summary {\n    /** current summary iteration, not bigger than 3 */\n    public int $iteration;\n    /** @var string[] 1-3 facts most relevant from executive perspective and missing from the summary (avoid technical details) */\n    public array $missingFacts;\n    /** denser summary in 1-3 sentences, which covers every fact from the previous summary plus the missing ones */\n    public string $expandedSummary;\n}\n\n/** Increasingly denser, expanded summaries */\nclass ChainOfSummaries {\n    /** simplified, executive view with no details, just a single statement of overall situation */\n    public string $overview;\n    /** @var Summary[] contains at least 3 gradually more expanded summaries of the content */\n    public array $summaries;\n}\n\n$summaries = (new Instructor)\n    -&gt;request(\n        messages: $report,\n        responseModel: ChainOfSummaries::class,\n        options: [\n            'max_tokens' =&gt; 4096,\n        ])\n    -&gt;get();\n\nprint(\"\\n# Summaries with increasing density:\\n\\n\");\nprint(\"Overview:\\n\");\nprint(\"{$summaries-&gt;overview}\\n\\n\");\nforeach ($summaries-&gt;summaries as $summary) {\n    print(\"Expanded summary - iteration #{$summary-&gt;iteration}:\\n\");\n    print(\"{$summary-&gt;expandedSummary}\\n\\n\");\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/chain_of_thought/","title":"Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality, by eliciting LLM reasoning to self-explain approach to generating the response.</p> <p>With Instructor you can achieve a 'modular' CoT, where multiple explanations can be generated by LLM for different parts of the response, driving a more granular control and improvement of the response.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass Employee {\n    /** Think step by step to determine the correct year of employment. */\n    public string $chainOfThought;\n    public int $yearOfEmployment;\n}\n\n$text = 'He was working here for 5 years. Now, in 2019, he is a manager.';\n\n$employee = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: Employee::class\n);\n\n\ndump($employee);\n\nassert($employee-&gt;yearOfEmployment === 2014);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification/","title":"Single label classification","text":""},{"location":"hub/techniques/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a PHP class for the output.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n// Enumeration for single-label text classification.\nenum Label : string {\n    case SPAM = \"spam\";\n    case NOT_SPAM = \"not_spam\";\n}\n\n// Class for a single class label prediction.\nclass SinglePrediction {\n    public Label $classLabel;\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification/#classifying-text","title":"Classifying Text","text":"<p>The function classify will perform the single-label classification.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction classify(string $data) : SinglePrediction {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Classify the following text: $data\",\n        ]],\n        responseModel: SinglePrediction::class,\n    );\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code>&lt;?php\n// Test single-label classification\n$prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\");\n\ndump($prediction);\n\nassert($prediction-&gt;classLabel == Label::SPAM);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification_multiclass/","title":"Multiclass classification","text":""},{"location":"hub/techniques/classification_multiclass/#defining-the-structures","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different PHP class to handle multiple labels.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/** Potential ticket labels */\nenum Label : string {\n    case TECH_ISSUE = \"tech_issue\";\n    case BILLING = \"billing\";\n    case SALES = \"sales\";\n    case SPAM = \"spam\";\n    case OTHER = \"other\";\n}\n\n/** Represents analysed ticket data */\nclass TicketLabels {\n    /** @var Label[] */\n    public array $labels = [];\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification_multiclass/#classifying-text","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> executes multi-label classification using LLM.</p> <pre><code>&lt;?php\n// Perform single-label classification on the input text.\nfunction multi_classify(string $data) : TicketLabels {\n    return (new Instructor())-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Label following support ticket: {$data}\",\n        ]],\n        responseModel: TicketLabels::class,\n    );\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/classification_multiclass/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code>&lt;?php\n// Test single-label classification\n$ticket = \"My account is locked and I can't access my billing info.\";\n$prediction = multi_classify($ticket);\n\ndump($prediction);\n\nassert(in_array(Label::TECH_ISSUE, $prediction-&gt;labels));\nassert(in_array(Label::BILLING, $prediction-&gt;labels));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/entity_relationships/","title":"Entity relationship extraction","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model.</p> <p>Following example demonstrates how to define relationships between users by incorporating an <code>$id</code> and <code>$coworkers</code> fields.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Enums\\Mode;\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    /** Unique identifier for each user. */\n    public int $id;\n    public int $age;\n    public string $name;\n    public string $role;\n    /**\n     * @var int[] Correct and complete list of coworker IDs, representing\n     * collaboration between users.\n     */\n    public array $coworkers;\n}\n\nclass UserRelationships\n{\n    /**\n     * @var UserDetail[] Collection of users, correctly capturing the\n     * relationships among them.\n     */\n    public array $users;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a Python programmer of Apex website.\n    Amanda is a contractor working with Jason on Apex website. John is\n    40yo and he's CEO - Jason reports to him.\n    TEXT;\n\n$relationships = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserRelationships::class,\n);\n\ndump($relationships);\n\nassert(!empty($relationships-&gt;users));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/handling_errors/","title":"Handling errors","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n}\n\nclass MaybeUser\n{\n    public ?UserDetail $user = null;\n    public bool $noUserData = false;\n    /** If no user data, provide reason */\n    public ?string $errorMessage = '';\n\n    public function get(): ?UserDetail\n    {\n        return $this-&gt;noUserData ? null : $this-&gt;user;\n    }\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; 'We don\\'t know anything about this guy.']],\n    responseModel: MaybeUser::class\n);\n\n\ndump($user);\n\nassert($user-&gt;noUserData);\nassert(!empty($user-&gt;errorMessage));\nassert($user-&gt;get() === null);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/image_to_data/","title":"Image to data","text":"<p>This is an example of how to extract structured data from an image using Instructor. The image is loaded from a file and converted to base64 format before sending it to OpenAI API.</p> <p>The response model is a PHP class that represents the structured receipt information with data of vendor, items, subtotal, tax, tip, and total.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Extras\\Image\\Image;\nuse Cognesy\\Instructor\\Instructor;\n\nclass Vendor {\n    public ?string $name = '';\n    public ?string $address = '';\n    public ?string $phone = '';\n}\n\nclass ReceiptItem {\n    public string $name;\n    public ?int $quantity = 1;\n    public float $price;\n}\n\nclass Receipt {\n    public Vendor $vendor;\n    /** @var ReceiptItem[] */\n    public array $items = [];\n    public ?float $subtotal;\n    public ?float $tax;\n    public ?float $tip;\n    public float $total;\n}\n\n$receipt = (new Instructor)-&gt;respond(\n    input: Image::fromFile(__DIR__ . '/receipt.png'),\n    responseModel: Receipt::class,\n    prompt: 'Extract structured data from the receipt.',\n    model: 'gpt-4-vision-preview',\n    options: ['max_tokens' =&gt; 4096]\n);\n\n\ndump($receipt);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/limiting_length_of_lists/","title":"Limiting the length of lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length of list. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <p>To be 100% certain the list does not exceed the limit, add extra validation, e.g. using ValidationMixin (see: Validation).</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\nuse Cognesy\\Instructor\\Validation\\Traits\\ValidationMixin;\nuse Cognesy\\Instructor\\Validation\\ValidationResult;\n\nclass Property\n{\n    /**  Monotonically increasing ID, not larger than 2 */\n    public string $index;\n    public string $key;\n    public string $value;\n}\n\nclass UserDetail\n{\n    use ValidationMixin;\n\n    public int $age;\n    public string $name;\n    /** @var Property[] List other extracted properties - not more than 2. */\n    public array $properties;\n\n    public function validate() : ValidationResult\n    {\n        if (count($this-&gt;properties) &lt; 3) {\n            return ValidationResult::valid();\n        }\n        return ValidationResult::fieldError(\n            field: 'properties',\n            value: $this-&gt;name,\n            message: \"Number of properties must be not more than 2.\",\n        );\n    }\n}\n\n$text = &lt;&lt;&lt;TEXT\n    Jason is 25 years old. He is a programmer. He has a car. He lives in\n    a small house in Alamo. He likes to play guitar.\n    TEXT;\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; $text]],\n    responseModel: UserDetail::class,\n    maxRetries: 1 // change to 0 to see validation error\n);\n\ndump($user);\n\nassert($user-&gt;age === 25);\nassert($user-&gt;name === \"Jason\");\nassert(count($user-&gt;properties) &lt; 3);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/restating_instructions/","title":"Restating instructions","text":"<p>Make Instructor restate long or complex instructions and rules to improve inference accuracy.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /** Restate instructions and rules, so you can correctly determine the title. */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /* Guess job title */\n    public string $title;\n}\n\n/**\n * Details of analyzed user. The key information we're looking for\n * is appropriate role data.\n */\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am the head of Apex Software, responsible for\n    driving growth of our company.\n    TEXT;\n\n$instructor = new Instructor;\n$user = ($instructor)-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n);\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\n//assert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/rewriting_instructions/","title":"Ask LLM to rewrite instructions","text":"<p>Asking LLM to rewrite the instructions and rules is another way to improve inference results.</p> <p>You can provide arbitrary instructions on the data handling in the class and property PHPDocs. Instructor will use these instructions to guide LLM in the inference process.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\n/**\n * Identify what kind of job the user is doing.\n * Typical roles we're working with are CEO, CTO, CFO, CMO.\n * Sometimes user does not state their role directly - you will need\n * to make a guess, based on their description.\n */\nclass UserRole\n{\n    /**\n     * Rewrite the instructions and rules in a concise form to correctly\n     * determine the user's title - just the essence.\n     */\n    public string $instructions;\n    /** Role description */\n    public string $description;\n    /** Most likely job title */\n    public string $title;\n}\n\nclass UserDetail\n{\n    public string $name;\n    public int $age;\n    public UserRole $role;\n}\n\n$text = &lt;&lt;&lt;TEXT\n    I'm Jason, I'm 28 yo. I am responsible for driving growth of our\n    company.\n    TEXT;\n\n$instructor = new Instructor;\n$user = $instructor-&gt;respond(\n    messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; $text]],\n    responseModel: UserDetail::class,\n);\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;age === 28);\nassert(!empty($user-&gt;role-&gt;title));\n?&gt;\n</code></pre>"},{"location":"hub/techniques/search_criteria/","title":"Expanding Search Queries","text":"<p>In this example, we will demonstrate how to leverage the enums and typed arrays to segment a complex search prompt into multiple, better structured queries that can be executed separately against specialized APIs or search engines.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use Instructor to segment search queries, so you can execute them separately against specialized APIs or search engines.</p>"},{"location":"hub/techniques/search_criteria/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>SearchQuery</code> class is a PHP class that defines the structure of an individual search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nenum SearchType : string {\n    case TEXT = \"text\";\n    case IMAGE = \"image\";\n    case VIDEO = \"video\";\n}\n\nclass Search\n{\n    /** @var SearchQuery[] */\n    public array $queries = [];\n}\n\nclass SearchQuery\n{\n    public string $title;\n    /**  Rewrite query for a search engine */\n    public string $query;\n    /** Type of search - image, video or text */\n    public SearchType $type;\n\n    public function execute() {\n        // ... write actual search code here\n        print(\"Searching for `{$this-&gt;title}` with query `{$this-&gt;query}` using `{$this-&gt;type-&gt;value}`\\n\");\n    }\n}\n?&gt;\n</code></pre>"},{"location":"hub/techniques/search_criteria/#segmenting-the-search-prompt","title":"Segmenting the Search Prompt","text":"<p>The <code>segment</code> function takes a string <code>data</code> and segments it into multiple search queries. It uses the <code>Instructor::respond</code> method to send a prompt and extract the data into the target object. The <code>responseModel</code> parameter specifies <code>Search::class</code> as the model to use for extraction.</p> <pre><code>&lt;?php\nfunction segment(string $data) : Search {\n    return (new Instructor)-&gt;respond(\n        messages: [[\n            \"role\" =&gt; \"user\",\n            \"content\" =&gt; \"Consider the data below: '\\n$data' and segment it into multiple search queries\",\n        ]],\n        responseModel: Search::class,\n    );\n}\n\n$search = segment(\"Find a picture of a cat and a video of a dog\");\nforeach ($search-&gt;queries as $query) {\n    $query-&gt;execute();\n}\n// Results:\n// Searching with query `picture of a cat` using `image`\n// Searching with query `video of a dog` using `video`\n\nassert(count($search-&gt;queries) === 2);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/time_range/","title":"Reusing components","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass TimeRange {\n    /** The start time in hours. */\n    public int $startTime;\n    /** The end time in hours. */\n    public int $endTime;\n}\n\nclass UserDetail\n{\n    public string $name;\n    /** Time range during which the user is working. */\n    public TimeRange $workTime;\n    /** Time range reserved for leisure activities. */\n    public TimeRange $leisureTime;\n}\n\n$user = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Yesterday Jason worked from 9 for 5 hours. Later I watched 2 hour movie which I finished at 19.\"]],\n    responseModel: UserDetail::class,\n    maxRetries: 2\n);\n\ndump($user);\n\nassert($user-&gt;name == \"Jason\");\nassert($user-&gt;workTime-&gt;startTime === 9);\nassert($user-&gt;workTime-&gt;endTime === 14);\nassert($user-&gt;leisureTime-&gt;startTime === 17);\nassert($user-&gt;leisureTime-&gt;endTime === 19);\n?&gt;\n</code></pre>"},{"location":"hub/techniques/time_range_with_cot/","title":"Using CoT to improve interpretation of component data","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both <code>$workTime</code> and <code>$leisureTime</code>.</p> <p>We're additionally starting the data structure with a Chain of Thought field to elicit LLM reasoning for the time range calculation, which can improve the accuracy of the response.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__.'../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass TimeRange\n{\n    /** Step by step reasoning to get the correct time range */\n    public string $chainOfThought;\n    /** The start time in hours (0-23 format) */\n    public int $startTime;\n    /** The end time in hours (0-23 format) */\n    public int $endTime;\n}\n\n$timeRange = (new Instructor)-&gt;respond(\n    messages: [['role' =&gt; 'user', 'content' =&gt; \"Workshop with Apex Industries started 9 and it took us 6 hours to complete.\"]],\n    responseModel: TimeRange::class,\n    maxRetries: 2\n);\n\ndump($timeRange);\n\nassert($timeRange-&gt;startTime === 9);\nassert($timeRange-&gt;endTime === 15);\n?&gt;\n</code></pre>"},{"location":"hub/troubleshooting/debugging/","title":"Debugging","text":"<p>Instructor gives you access to SaloonPHP debugging mode by setting <code>options</code> array key <code>debug</code> to <code>true</code> when creating a client instance.</p> <p>Setting <code>debug</code> option to true causes underlying SaloonPHP library to output HTTP request and response details to the console, so you can see what is being sent to LLM API and what is being received.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nclass User {\n    public int $age;\n    public string $name;\n}\n\necho \"Debugging request and response:\\n\\n\";\n$user = (new Instructor)-&gt;withDebug()-&gt;respond(\n    messages: \"Jason is 25 years old.\",\n    responseModel: User::class,\n);\n\necho \"\\nResult:\\n\";\ndump($user);\n\nassert(isset($user-&gt;name));\nassert(isset($user-&gt;age));\n?&gt;\n</code></pre>"},{"location":"hub/troubleshooting/on_error/","title":"Handle processing errors","text":"<p><code>Instructor-&gt;onError(callable $callback)</code> method allows you to receive callback on any uncaught error, so you can customize handling it, for example logging the error or using some fallback mechanism in an attempt to recover.</p> <p>In case Instructor encounters any error that it cannot handle, your callable (if defined) will be called with an instance of <code>ErrorRaised</code> event, which contains information about the error and request that caused it (among some other properties).</p> <p>In most cases, after you process the error (e.g. store it in a log via some logger) the best way to proceed is to rethrow the error.</p> <p>If you do not rethrow the error and just return some value, Instructor will return it as a result of response processing. This way you can provide a fallback response, e.g. with an object with default values.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Clients\\Mistral\\MistralClient;\nuse Cognesy\\Instructor\\Events\\Instructor\\ErrorRaised;\nuse Cognesy\\Instructor\\Instructor;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n// let's mock a logger class - in real life you would use a proper logger, e.g. Monolog\n$logger = new class {\n    public function error(ErrorRaised $event) {\n        // instead of logging we will print the data to the console\n        echo $event-&gt;asLog().\"\\n\";\n        // normally, you'd want to rethrow the error here\n    }\n};\n\n// we will intentionally create an error by providing a wrong client credentials\n$client = new MistralClient('wrong-id', 'wrong-uri');\n\n$user = (new Instructor)\n    -&gt;withClient($client)\n    -&gt;request(\n        messages: \"Jason is 28 years old\",\n        responseModel: User::class,\n    )\n    -&gt;onError(fn(ErrorRaised $event) =&gt; $logger-&gt;error($event))\n    -&gt;get();\n\ndump($user);\n?&gt;\n</code></pre>"},{"location":"hub/troubleshooting/on_event/","title":"Receive specific internal events","text":"<p><code>(new Instructor)-&gt;onEvent(string $class, callable $callback)</code> method allows you to receive callback when specified type of event is dispatched by Instructor.</p> <p>This way you can plug into the execution process and monitor it, for example logging or reacting to the events which are of interest to your application.</p> <p>This example demonstrates how you can monitor outgoing requests and received responses via Instructor's events.</p> <p>Check the <code>Cognesy\\Instructor\\Events</code> namespace for the list of available events and their properties.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Events\\Event;\nuse Cognesy\\Instructor\\Events\\Request\\RequestSentToLLM;\nuse Cognesy\\Instructor\\Events\\Request\\ResponseReceivedFromLLM;\nuse Cognesy\\Instructor\\Instructor;\n\nclass User\n{\n    public string $name;\n    public int $age;\n}\n\n// let's mock a logger class - in real life you would use a proper logger, e.g. Monolog\n$logger = new class {\n    public function log(Event $event) {\n        // we're using a predefined asLog() method to get the event data,\n        // but you can access the event properties directly and customize the output\n        echo $event-&gt;asLog().\"\\n\";\n    }\n};\n\n$user = (new Instructor)\n    -&gt;request(\n        messages: \"Jason is 28 years old\",\n        responseModel: User::class,\n    )\n    -&gt;onEvent(RequestSentToLLM::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;onEvent(ResponseReceivedFromLLM::class, fn($event) =&gt; $logger-&gt;log($event))\n    -&gt;get();\n\ndump($user);\n?&gt;\n</code></pre>"},{"location":"hub/troubleshooting/wiretap/","title":"Receive all internal events with wiretap()","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <p><code>(new Instructor)-&gt;wiretap(callable $callback)</code> method allows you to receive all events dispatched by Instructor.</p> <p>Example below demonstrates how <code>wiretap()</code> can help you to monitor the execution process and better understand or resolve any processing issues.</p> <p>In this example we use <code>print()</code> method available on event classes, which outputs console-formatted information about each event.</p> <pre><code>&lt;?php\n$loader = require 'vendor/autoload.php';\n$loader-&gt;add('Cognesy\\\\Instructor\\\\', __DIR__ . '../../src/');\n\nuse Cognesy\\Instructor\\Instructor;\n\nenum Role : string {\n    case CEO = 'ceo';\n    case CTO = 'cto';\n    case Developer = 'developer';\n    case Other = 'other';\n}\n\nclass UserDetail\n{\n    public string $name;\n    public Role $role;\n    public int $age;\n}\n\n$user = (new Instructor)\n    -&gt;request(\n        messages: [[\"role\" =&gt; \"user\",  \"content\" =&gt; \"Contact our CTO, Jason is 28 years old -- Best regards, Tom\"]],\n        responseModel: UserDetail::class,\n        options: ['stream' =&gt; true]\n    )\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    -&gt;get();\n\ndump($user);\n\nassert($user-&gt;name === \"Jason\");\nassert($user-&gt;role === Role::CTO);\nassert($user-&gt;age === 28);\n?&gt;\n</code></pre>"},{"location":"internals/","title":"Internals","text":"<p>You can find some information on Instructor internals here, which should allow you better understand how it is designed and how it works.</p> <ul> <li>Instructor</li> <li>Configuration</li> <li>Lifecycle</li> <li>Response Models</li> <li>Contracts</li> <li>Events</li> <li>HTTP client</li> <li>Debugging</li> </ul>"},{"location":"internals/configuration/","title":"Configuration","text":"<p>Instructor uses <code>Configuration</code> and <code>ConmponentConfig</code> classes to handle configuration of all components of the library.</p>"},{"location":"internals/configuration/#configuration-class","title":"<code>Configuration</code> class","text":"<p>Instructor uses <code>Configuration</code> class to handle configuration of all components of the library. It is used to define components, the way to instantiate them and the dependencies between components.</p> <p><code>Configuration</code> class is responsible for instantiation of the components (and inject them with the configuration data).</p>"},{"location":"internals/configuration/#componentconfig-class","title":"<code>ComponentConfig</code> class","text":"<p><code>ComponentConfig</code> class contains a configuration data of a single component.</p>"},{"location":"internals/configuration/#global-autowire-function","title":"Global <code>autowire()</code> function","text":"<p>Instructor comes with a global <code>autowire()</code> function containing a default wiring between components used by the library. It is located in <code>config\\autowire.php</code> file.</p>"},{"location":"internals/contracts/","title":"Response model contracts","text":"<p>Instructor allows you to customize processing of $responseModel value also by looking at the interfaces the class or instance implements:</p> <ul> <li><code>CanProvideJsonSchema</code> - implement to be able to provide raw JSON Schema (as an array) of the response model, overriding the default approach of Instructor, which is analyzing $responseModel value class information,</li> <li><code>CanProvideSchema</code> - implement to be able to provide <code>Schema</code> object of the response model, overriding class analysis stage; can be useful in building object wrappers (see: <code>Sequence</code> class),</li> <li><code>CanDeserializeSelf</code> - implement to customize the way the response from LLM is deserialized from JSON into PHP object,</li> <li><code>CanValidateSelf</code> - implement to customize the way the deserialized object is validated - it fully replaces the default validation process for given response model,</li> <li><code>CanTransformSelf</code> - implement to transform the validated object into any target value that will be then passed back to the caller (e.g. unwrap simple type from a class to scalar value)</li> </ul> <p>Methods implemented by those interfaces are executed is following:  -   - CanProvideJsonSchema - executed during the schema generation phase,  - CanDeserializeSelf - executed during the deserialization phase,  - CanValidateSelf - executed during the validation phase,  - CanTransformSelf - executed during the transformation phase.</p> <p>When implementing custom response handling strategy, avoid doing all transformations in a single block of code. Split the logic between relevant methods implemented by your class for clarity and easier code maintenance.</p>"},{"location":"internals/contracts/#example-implementation","title":"Example implementation","text":"<p>For a practical example of using those contracts to customize Instructor processing flow see:</p> <ul> <li>src/Extras/Scalar/</li> <li>src/Extras/Sequence/</li> </ul> <p>Examples contain an implementation of custom $responseModel wrappers, e.g. providing scalar value response support with a wrapper class implementing custom schema provider, deserialization, validation and transformation into requested value type.</p>"},{"location":"internals/debugging/","title":"Debugging","text":""},{"location":"internals/debugging/#events","title":"Events","text":"<p>Instructor emits events at various points in its lifecycle, which you can listen to and react to. You can use these events to debug execution flow and to inspect data at various stages of processing.</p> <p>For more details see the Events section.</p>"},{"location":"internals/debugging/#http-debugging","title":"HTTP Debugging","text":"<p>Instructor gives you access to SaloonPHP debugging mode by calling <code>withDebug()</code> method on Instructor instance. For example:</p> <pre><code>$result = (new Instructor)-&gt;withDebug()-&gt;respond(\n    messages: \"Jason is 25 years old\",\n    responseModel: User:class,\n);\n</code></pre> <p>Setting debug mode causes underlying SaloonPHP library to output HTTP request and response details to the console, so you can see what is being sent to LLM API and what is being received.</p> <p>You can also directly access Saloon connector instance via <code>connector()</code> method on the client instance, and call Saloon debugging methods on it - see SaloonPHP debugging documentation for more details: https://docs.saloon.dev/the-basics/debugging</p> <p>Additionally, <code>connector()</code> method on the client instance allows you to access other capabilities of Saloon connector, such as setting or modifying middleware. See SaloonPHP documentation for more details: https://docs.saloon.dev/digging-deeper/middleware</p>"},{"location":"internals/events/","title":"Events","text":""},{"location":"internals/events/#event-classes","title":"Event classes","text":"<p>Instructor dispatches multiple classes of events (all inheriting from <code>Event</code> class) during its execution. You can listen to these events and react to them in your application, for example to log information or to monitor the execution process.</p> <p>Check the list of available event classes in the <code>Cognesy\\Instructor\\Events</code> namespace.</p>"},{"location":"internals/events/#event-methods","title":"Event methods","text":"<p>Each Instructor event offers following methods, which make interacting with them more convenient:</p> <ul> <li><code>print()</code> - prints a string representation of the event to console output</li> <li><code>asConsole()</code> - returns the event in a format suitable for console output</li> <li><code>asLog()</code> - returns the event in a format suitable for logging</li> </ul>"},{"location":"internals/events/#receiving-notification-on-internal-events","title":"Receiving notification on internal events","text":"<p>Instructor allows you to receive detailed information at every stage of request and response processing via events.</p> <ul> <li><code>(new Instructor)-&gt;onEvent(string $class, callable $callback)</code> method - receive callback when specified type of event is dispatched</li> <li><code>(new Instructor)-&gt;wiretap(callable $callback)</code> method - receive any event dispatched by Instructor, may be useful for debugging or performance analysis</li> <li><code>(new Instructor)-&gt;onError(callable $callback)</code> method - receive callback on any uncaught error, so you can customize handling it, for example logging the error</li> </ul> <p>Receiving events can help you to monitor the execution process and makes it easier for a developer to understand and resolve any processing issues.</p> <pre><code>$instructor = (new Instructor)\n    // see requests to LLM\n    -&gt;onEvent(RequestSentToLLM::class, fn($e) =&gt; dump($e))\n    // see responses from LLM\n    -&gt;onEvent(ResponseReceivedFromLLM::class, fn($event) =&gt; dump($event))\n    // see all events in console-friendly format\n    -&gt;wiretap(fn($event) =&gt; $event-&gt;print())\n    // log all events in log-friendly format\n    -&gt;wiretap(fn($event) =&gt; YourLogger::log($event-&gt;asLog()))\n    // log errors via your custom logger\n    -&gt;onError(fn($request, $error) =&gt; YourLogger::log($error)));\n\n$instructor-&gt;respond(\n    messages: \"What is the population of Paris?\",\n    responseModel: Scalar::integer(),\n);\n// check your console for the details on the Instructor execution\n</code></pre>"},{"location":"internals/events/#onerror-handler","title":"onError handler","text":"<p>In case Instructor encounters any error that it cannot handle, your callable (if defined) will be called with an instance of <code>ErrorRaised</code> event, which contains information about the error and request that caused it (among some other properties).</p> <p><code>Instructor-&gt;onError(callable $callback)</code> method allows you to receive callback on any uncaught error, so you can customize handling it, for example to log the error.</p> <p>If you do not rethrow the error and just return some value, Instructor will return it as a result of response processing. This way you can provide a fallback response, e.g. with an object with default values.</p> <p>NOTE: In almost all situations, the best way to proceed after you process the error (e.g. store it in a log) is to rethrow the error. Make sure you clearly understand what you're doing if you choose not to rethrow the exception from your handler. </p>"},{"location":"internals/events/#convenience-methods-for-get-streamed-model-updates","title":"Convenience methods for get streamed model updates","text":"<p><code>Instructor</code> class provides convenience methods allowing client code to receive model updates  when streaming is enabled:</p> <ul> <li><code>onPartialUpdate(callable $callback)</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate(callable $callback)</code> - to handle partial sequence updates of the response</li> </ul> <p>In both cases your callback will receive updated model, so you don't have to extract it from the event.</p>"},{"location":"internals/http/","title":"HTTP Connectivity","text":"<p>Instructor uses SaloonPHP as its HTTP connectivity layer.</p> <p>SaloonPHP is a lightweight HTTP client library that provides a simple API for making HTTP API requests and handling responses. It is designed to be easy to use and flexible, with a focus on performance and reliability.</p> <p>SaloonPHP is built on top of the popular Guzzle</p>"},{"location":"internals/http/#accessing-saloonphp-connector","title":"Accessing SaloonPHP connector","text":"<p>You can directly access Saloon connector instance via <code>connector()</code> method on the client instance, and call Saloon debugging methods on it - see SaloonPHP debugging documentation for more details: https://docs.saloon.dev/the-basics/debugging</p>"},{"location":"internals/http/#request-and-response-middleware","title":"Request and response middleware","text":"<p>Additionally, <code>connector()</code> method on the client instance allows you to access other capabilities of Saloon connector, such as adding middleware via <code>onRequest()</code> or <code>onResponse()</code> methods on the Connector instance returned by <code>connector()</code>. See SaloonPHP documentation for more details: https://docs.saloon.dev/digging-deeper/middleware</p>"},{"location":"internals/http/#custom-connectors","title":"Custom connectors","text":"<p>Instructor allows you to provide custom connector to the client instance via <code>withConnector()</code> method on the API client instance. Provided connector must inherit from <code>Cognesy\\Instructor\\ApiClient\\ApiConnector</code>.</p>"},{"location":"internals/http/#customizing-http-client-used-by-saloonphp","title":"Customizing HTTP client used by SaloonPHP","text":"<p>SaloonPHP uses Guzzle as its HTTP client by default. You can customize HTTP sender used by SaloonPHP by providing custom <code>senderClass</code> parameter to your connector constructor, which should be a class name implementing Saloon's <code>Sender</code> contract: https://github.com/saloonphp/saloon/blob/v3/src/Contracts/Sender.php</p>"},{"location":"internals/instructor/","title":"<code>Instructor</code> class","text":"<p><code>Instructor</code> class is the main entry point to the library. It is responsible for handling all interactions with the client code and internal Instructor components.</p> <p>One of the essential tasks of the <code>Instructor</code> class is to read the configuration and use it to retrieve a component implementing <code>CanHandleRequest</code> interface (specified in the configuration) to process the request and return the response.</p> <p><code>Instructor</code> class dispatches several high level events during initialization and processing of the request and response:</p> <ul> <li><code>InstructorStarted</code> - dispatched when Instructor is created</li> <li><code>InstructorReady</code> - dispatched when Instructor is configured and ready to process the request</li> <li><code>RequestReceived</code> - dispatched when the request is received</li> <li><code>ResponseGenerated</code> - dispatched when the response is generated</li> <li><code>ErrorRaised</code> - dispatched when an uncaught error occurs</li> </ul> <p><code>Instructor</code> class contains top level try-catch block to let user handle any uncaught errors before throwing them to the client code.</p> <p><code>Instructor</code> class provides several methods allowing client code to plug into Instructor event system, including:  - <code>onEvent()</code> - to receive a callback when specified type of event is dispatched  - <code>wiretap()</code> - to receive any event dispatched by Instructor  - <code>onError()</code> - to receive callback on any uncaught error</p> <p>Additionally, <code>Instructor</code> class provides convenience methods allowing client code to receive model updates when streaming is enabled:</p> <ul> <li><code>onPartialUpdate()</code> - to handle partial model updates of the response</li> <li><code>onSequenceUpdate()</code> - to handle partial sequence updates of the response</li> </ul>"},{"location":"internals/lifecycle/","title":"Lifecycle","text":"<p>As Instructor for PHP processes your request, it goes through several stages:</p> <ol> <li>Initialize and self-configure (with possible overrides defined by developer).</li> <li>Analyze classes and properties of the response data model specified by developer.</li> <li>Encode data model into a schema that can be provided to LLM.</li> <li>Execute request to LLM using specified messages (content) and response model metadata.</li> <li>Receive a response from LLM or multiple partial responses (if streaming enabled).</li> <li>Deserialize response received from LLM into originally requested classes and their properties.</li> <li>In case response contained incomplete or corrupted data - if errors are encountered, create feedback message for LLM and requests regeneration of the response.</li> <li>Execute validations defined by developer for the data model - if any of them fail, create feedback message for LLM and requests regeneration of the response.</li> <li>Repeat the steps 4-8, unless specified limit of retries has been reached or response passes validation</li> </ol>"},{"location":"internals/response_models/","title":"Response Models","text":"<p>Instructor is able to process several types of input provided as response model, giving you more flexibility on how you interact with the library.</p> <p>The signature of <code>respond()</code> method of Instructor states the <code>responseModel</code> can be either string, object or array.</p>"},{"location":"internals/response_models/#handling-string-responsemodel-value","title":"Handling string $responseModel value","text":"<p>If <code>string</code> value is provided, it is used as a name of the class of the response model.</p> <p>Instructor checks if the class exists and analyzes the class &amp; properties type information &amp; doc comments to generate a schema needed to specify LLM response constraints.</p> <p>The best way to provide the name of the response model class is to use <code>NameOfTheClass::class</code>, making it easy for IDE to check the type, handle refactorings, etc.</p>"},{"location":"internals/response_models/#handling-object-responsemodel-value","title":"Handling object $responseModel value","text":"<p>If <code>object</code> value is provided, it is considered an instance of the response model. Instructor checks the class of the instance, then analyzes it and its property type data to specify LLM response constraints.</p>"},{"location":"internals/response_models/#handling-array-responsemodel-value","title":"Handling array $responseModel value","text":"<p>If <code>array</code> value is provided, it is considered a raw JSON Schema, therefore allowing Instructor to use it directly in LLM requests (after wrapping in appropriate context - e.g. function call).</p> <p>Instructor requires information on the class of each nested object in your JSON Schema, so it can correctly deserialize the data into appropriate type.</p> <p>This information is available to Instructor when you are passing $responseModel as a class name or an instance, but it is missing from raw JSON Schema. Lack of the information on target class makes it impossible for Instructor to deserialize the data into appropriate, expected type.</p> <p>Current design uses JSON Schema <code>$comment</code> field on property to overcome this information gap. Instructor expects developer to use <code>$comment</code> field to provide fully qualified name of the target class to be used to deserialize property data of object or enum type.</p>"}]}